"""
Djangoç®¡ç†å‘½ä»¤ï¼šç¬¬ä¸€é˜¶æ®µ - åªæå–å‚è€ƒæ–‡çŒ®åŸå§‹æ–‡æœ¬
ä»ArXivè®ºæ–‡PDFä¸­æå–å‚è€ƒæ–‡çŒ®åŸå§‹æ–‡æœ¬å¹¶å­˜å‚¨åˆ°æ•°æ®åº“ï¼Œä¸è°ƒç”¨LLM
"""
from django.core.management.base import BaseCommand, CommandError
from django.db import transaction
from django.utils import timezone
from datetime import datetime, timedelta
import logging
import time

from core.arxiv_models import ArxivPaper, ArxivReferenceExtractLog
from core.arxiv_reference_extractor import ArxivReferenceExtractor


# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = 'ç¬¬ä¸€é˜¶æ®µï¼šä»ArXivè®ºæ–‡PDFä¸­æå–å‚è€ƒæ–‡çŒ®åŸå§‹æ–‡æœ¬ï¼ˆä¸è°ƒç”¨LLMï¼‰'
    
    def add_arguments(self, parser):
        parser.add_argument(
            '--arxiv-id',
            type=str,
            help='å¤„ç†æŒ‡å®šçš„arXiv ID'
        )
        parser.add_argument(
            '--limit',
            type=int,
            default=None,
            help='é™åˆ¶å¤„ç†çš„è®ºæ–‡æ•°é‡'
        )
        parser.add_argument(
            '--batch-size',
            type=int,
            default=10,
            help='æ‰¹é‡å¤„ç†çš„å¤§å°'
        )
        parser.add_argument(
            '--skip-existing',
            action='store_true',
            help='è·³è¿‡å·²ç»æå–è¿‡å‚è€ƒæ–‡çŒ®æ–‡æœ¬çš„è®ºæ–‡'
        )
        parser.add_argument(
            '--retry-failed',
            action='store_true',
            help='é‡è¯•ä¹‹å‰å¤±è´¥çš„è®ºæ–‡'
        )
        parser.add_argument(
            '--max-retries',
            type=int,
            default=3,
            help='å•ç¯‡è®ºæ–‡çš„æœ€å¤§é‡è¯•æ¬¡æ•°'
        )
        parser.add_argument(
            '--delay',
            type=float,
            default=0.5,
            help='æ¯ç¯‡è®ºæ–‡å¤„ç†ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰'
        )
    
    def handle(self, *args, **options):
        self.stdout.write(self.style.SUCCESS('ã€ç¬¬ä¸€é˜¶æ®µã€‘å¼€å§‹æå–ArXivè®ºæ–‡å‚è€ƒæ–‡çŒ®åŸå§‹æ–‡æœ¬...'))
        
        # åˆå§‹åŒ–æå–å™¨
        try:
            extractor = ArxivReferenceExtractor(
                max_retries=options['max_retries']
            )
            self.stdout.write('æå–å™¨å·²åˆå§‹åŒ–ï¼ˆæ— éœ€LLMé…ç½®ï¼‰')
        except Exception as e:
            raise CommandError(f'åˆå§‹åŒ–æå–å™¨å¤±è´¥: {str(e)}')
        
        # è·å–éœ€è¦å¤„ç†çš„è®ºæ–‡
        papers = self._get_papers_to_process(options)
        
        if not papers:
            self.stdout.write(self.style.WARNING('æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„è®ºæ–‡'))
            return
        
        total = len(papers)
        self.stdout.write(f'æ‰¾åˆ° {total} ç¯‡è®ºæ–‡å¾…å¤„ç†')
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total': total,
            'processed': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0,
        }
        
        # æ‰¹é‡å¤„ç†
        batch_size = options['batch_size']
        delay = options['delay']
        
        for i in range(0, total, batch_size):
            batch = papers[i:i + batch_size]
            self.stdout.write(f'\nå¤„ç†æ‰¹æ¬¡ {i // batch_size + 1} (è®ºæ–‡ {i + 1}-{min(i + batch_size, total)})')
            
            for paper in batch:
                result = self._process_single_paper(paper, extractor, options)
                
                # æ›´æ–°ç»Ÿè®¡
                stats['processed'] += 1
                if result == 'success':
                    stats['success'] += 1
                elif result == 'failed':
                    stats['failed'] += 1
                elif result == 'skipped':
                    stats['skipped'] += 1
                
                # å»¶è¿Ÿ
                if delay > 0 and stats['processed'] < total:
                    time.sleep(delay)
            
            # æ˜¾ç¤ºè¿›åº¦
            self._print_progress(stats)
        
        # æœ€ç»ˆç»Ÿè®¡
        self.stdout.write('\n' + '=' * 60)
        self.stdout.write(self.style.SUCCESS('ã€ç¬¬ä¸€é˜¶æ®µã€‘å¤„ç†å®Œæˆï¼'))
        self._print_final_stats(stats)
    
    def _get_papers_to_process(self, options):
        """è·å–éœ€è¦å¤„ç†çš„è®ºæ–‡åˆ—è¡¨"""
        queryset = ArxivPaper.objects.all()
        
        # å¦‚æœæŒ‡å®šäº†arxiv_id
        if options['arxiv_id']:
            queryset = queryset.filter(arxiv_id=options['arxiv_id'])
        
        # å¦‚æœè·³è¿‡å·²å¤„ç†çš„
        if options['skip_existing']:
            # è·å–å·²ç»æå–äº†å‚è€ƒæ–‡çŒ®æ–‡æœ¬çš„è®ºæ–‡ID
            processed_ids = ArxivReferenceExtractLog.objects.filter(
                reference_section_found=True,
                reference_raw_text__isnull=False
            ).values_list('paper_id', flat=True)
            
            queryset = queryset.exclude(id__in=processed_ids)
        
        # å¦‚æœåªé‡è¯•å¤±è´¥çš„
        if options['retry_failed']:
            failed_ids = ArxivReferenceExtractLog.objects.filter(
                status='failed'
            ).values_list('paper_id', flat=True)
            
            queryset = queryset.filter(id__in=failed_ids)
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
        queryset = queryset.order_by('-published')
        
        # åº”ç”¨é™åˆ¶
        if options['limit']:
            queryset = queryset[:options['limit']]
        
        return list(queryset)
    
    def _process_single_paper(self, paper, extractor, options):
        """å¤„ç†å•ç¯‡è®ºæ–‡"""
        arxiv_id = paper.arxiv_id
        
        self.stdout.write(f'\n{"="*70}')
        self.stdout.write(f'å¤„ç†è®ºæ–‡: {arxiv_id}')
        self.stdout.write(f'æ ‡é¢˜: {paper.title[:60]}...')
        self.stdout.write(f'{"="*70}')
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æå–è®°å½•
        if options['skip_existing']:
            existing_log = ArxivReferenceExtractLog.objects.filter(
                paper=paper,
                reference_section_found=True,
                reference_raw_text__isnull=False
            ).first()
            
            if existing_log:
                self.stdout.write(self.style.WARNING(f'  âŠ™ è·³è¿‡ï¼ˆå·²æå–æ–‡æœ¬ï¼‰'))
                return 'skipped'
        
        # æ£€æŸ¥é‡è¯•æ¬¡æ•°
        retry_count = ArxivReferenceExtractLog.objects.filter(
            paper=paper
        ).count()
        
        if retry_count >= options['max_retries'] and not options['retry_failed']:
            self.stdout.write(self.style.WARNING(f'  âŠ™ è·³è¿‡ï¼ˆè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰'))
            return 'skipped'
        
        # åˆ›å»ºæå–æ—¥å¿—
        log = ArxivReferenceExtractLog.objects.create(
            paper=paper,
            status='extracting',
            retry_count=retry_count
        )
        
        start_time = timezone.now()
        
        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(step_name, details):
            """æ‰“å°å¤„ç†è¿›åº¦"""
            step_icons = {
                'downloading': 'ğŸ“¥',
                'downloaded': 'âœ…',
                'download_failed': 'âŒ',
                'extracting_text': 'ğŸ“',
                'text_extracted': 'âœ…',
                'extraction_failed': 'âŒ',
                'finding_references': 'ğŸ”',
                'references_found': 'âœ…',
                'reference_not_found': 'âš ï¸',
            }
            icon = step_icons.get(step_name, 'â€¢')
            self.stdout.write(f'  {icon} {details}')
        
        try:
            # æ‰§è¡Œæå–ï¼ˆåªæå–æ–‡æœ¬ï¼Œä¸è°ƒç”¨LLMï¼‰
            result = extractor.extract_reference_text_only(
                paper_id=paper.id,
                arxiv_id=arxiv_id,
                pdf_url=paper.pdf_url,
                progress_callback=progress_callback
            )
            
            # æ›´æ–°æ—¥å¿—
            log.pdf_downloaded = result['pdf_downloaded']
            log.pdf_file_path = result['pdf_path']
            log.pdf_file_size = result['pdf_size']
            log.text_extracted = result['text_extracted']
            log.reference_section_found = result['reference_section_found']
            
            if result['success']:
                log.status = 'completed'
                log.reference_raw_text = result['reference_text']
                log.reference_text_length = result['reference_text_length']
                log.completed_at = timezone.now()
                log.duration_seconds = (log.completed_at - start_time).seconds
                log.save()
                
                self.stdout.write(self.style.SUCCESS(
                    f'  âœ“ æå–å®Œæˆï¼å‚è€ƒæ–‡çŒ®æ–‡æœ¬é•¿åº¦: {result["reference_text_length"]} å­—ç¬¦ï¼Œè€—æ—¶ {log.duration_seconds} ç§’'
                ))
                return 'success'
            else:
                log.status = 'failed'
                log.error_type = result['error_type']
                log.error_message = result['error_message']
                log.completed_at = timezone.now()
                log.duration_seconds = (log.completed_at - start_time).seconds
                log.save()
                
                self.stdout.write(self.style.ERROR(
                    f'  âœ— æå–å¤±è´¥: {result["error_type"]}'
                ))
                self.stdout.write(self.style.ERROR(
                    f'     é”™è¯¯è¯¦æƒ…: {result["error_message"][:200]}'
                ))
                return 'failed'
                
        except Exception as e:
            log.status = 'failed'
            log.error_type = 'unexpected_error'
            log.error_message = str(e)
            log.completed_at = timezone.now()
            log.duration_seconds = (log.completed_at - start_time).seconds
            log.save()
            
            self.stdout.write(self.style.ERROR(f'  âœ— å¼‚å¸¸: {str(e)}'))
            logger.exception(f'å¤„ç†è®ºæ–‡ {arxiv_id} æ—¶å‘ç”Ÿå¼‚å¸¸')
            return 'failed'
    
    def _print_progress(self, stats):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        self.stdout.write(
            f"\nè¿›åº¦: {stats['processed']}/{stats['total']} | "
            f"æˆåŠŸ: {stats['success']} | "
            f"å¤±è´¥: {stats['failed']} | "
            f"è·³è¿‡: {stats['skipped']}"
        )
    
    def _print_final_stats(self, stats):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        # è®¡ç®—å·²æå–æ–‡æœ¬çš„è®°å½•æ•°
        total_extracted = ArxivReferenceExtractLog.objects.filter(
            reference_section_found=True,
            reference_raw_text__isnull=False
        ).count()
        
        self.stdout.write(f'æ€»è®ºæ–‡æ•°: {stats["total"]}')
        self.stdout.write(self.style.SUCCESS(f'æˆåŠŸ: {stats["success"]}'))
        if stats['failed'] > 0:
            self.stdout.write(self.style.ERROR(f'å¤±è´¥: {stats["failed"]}'))
        if stats['skipped'] > 0:
            self.stdout.write(self.style.WARNING(f'è·³è¿‡: {stats["skipped"]}'))
        self.stdout.write(f'æ•°æ®åº“ä¸­å·²æå–å‚è€ƒæ–‡çŒ®æ–‡æœ¬çš„è®°å½•æ•°: {total_extracted}')
        
        # æˆåŠŸç‡
        if stats['processed'] > 0:
            success_rate = (stats['success'] / stats['processed']) * 100
            self.stdout.write(f'æˆåŠŸç‡: {success_rate:.1f}%')
