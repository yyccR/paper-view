"""
Djangoç®¡ç†å‘½ä»¤ï¼šæå–ArXivè®ºæ–‡çš„å‚è€ƒæ–‡çŒ®
ä»ArXivè®ºæ–‡æ•°æ®è¡¨ä¸­è¯»å–è®ºæ–‡ï¼Œä¸‹è½½PDFï¼Œæå–å‚è€ƒæ–‡çŒ®å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
"""
from django.core.management.base import BaseCommand, CommandError
from django.db import transaction
from django.utils import timezone
from datetime import datetime, timedelta
import logging
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

from core.arxiv_models import ArxivPaper, ArxivPaperReference, ArxivReferenceExtractLog
from core.arxiv_reference_extractor import ArxivReferenceExtractor


# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class ThreadSafeStats:
    """çº¿ç¨‹å®‰å…¨çš„ç»Ÿè®¡è®¡æ•°å™¨"""
    def __init__(self, total):
        self.total = total
        self.processed = 0
        self.success = 0
        self.failed = 0
        self.skipped = 0
        self.lock = threading.Lock()
    
    def update(self, result):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            self.processed += 1
            if result == 'success':
                self.success += 1
            elif result == 'failed':
                self.failed += 1
            elif result == 'skipped':
                self.skipped += 1
    
    def get_dict(self):
        """è·å–ç»Ÿè®¡å­—å…¸ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.lock:
            return {
                'total': self.total,
                'processed': self.processed,
                'success': self.success,
                'failed': self.failed,
                'skipped': self.skipped,
            }


class Command(BaseCommand):
    help = 'ä»ArXivè®ºæ–‡PDFä¸­æå–å‚è€ƒæ–‡çŒ®å¹¶å­˜å‚¨åˆ°æ•°æ®åº“'
    
    def add_arguments(self, parser):
        parser.add_argument(
            '--mode',
            type=str,
            choices=['extract', 'process', 'full'],
            default='full',
            help='å¤„ç†æ¨¡å¼ï¼šextractï¼ˆåªæå–æ–‡æœ¬ï¼‰ã€processï¼ˆåªLLMå¤„ç†ï¼‰ã€fullï¼ˆå®Œæ•´æµç¨‹ï¼Œé»˜è®¤ï¼‰'
        )
        parser.add_argument(
            '--arxiv-id',
            type=str,
            help='å¤„ç†æŒ‡å®šçš„arXiv ID'
        )
        parser.add_argument(
            '--limit',
            type=int,
            default=None,
            help='é™åˆ¶å¤„ç†çš„è®ºæ–‡æ•°é‡'
        )
        parser.add_argument(
            '--batch-size',
            type=int,
            default=10,
            help='æ‰¹é‡å¤„ç†çš„å¤§å°'
        )
        parser.add_argument(
            '--skip-existing',
            action='store_true',
            help='è·³è¿‡å·²ç»æå–è¿‡å‚è€ƒæ–‡çŒ®çš„è®ºæ–‡'
        )
        parser.add_argument(
            '--retry-failed',
            action='store_true',
            help='é‡è¯•ä¹‹å‰å¤±è´¥çš„è®ºæ–‡'
        )
        parser.add_argument(
            '--max-retries',
            type=int,
            default=3,
            help='å•ç¯‡è®ºæ–‡çš„æœ€å¤§é‡è¯•æ¬¡æ•°'
        )
        parser.add_argument(
            '--llm-provider',
            type=str,
            default='qwen',
            help='LLMæä¾›å•†ï¼ˆé»˜è®¤: qwenï¼‰'
        )
        parser.add_argument(
            '--llm-model',
            type=str,
            default='qwen3-max',
            help='LLMæ¨¡å‹åç§°ï¼ˆé»˜è®¤: qwen3-maxï¼‰'
        )
        parser.add_argument(
            '--llm-timeout',
            type=int,
            default=600,
            help='LLM APIè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤: 600ï¼‰'
        )
        parser.add_argument(
            '--delay',
            type=float,
            default=1.0,
            help='æ¯ç¯‡è®ºæ–‡å¤„ç†ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰'
        )
        parser.add_argument(
            '--cleanup-pdfs',
            type=int,
            default=None,
            help='æ¸…ç†Nå¤©å‰çš„PDFæ–‡ä»¶'
        )
        # Process æ¨¡å¼ç‰¹æœ‰å‚æ•°
        parser.add_argument(
            '--skip-processed',
            action='store_true',
            help='ï¼ˆprocessæ¨¡å¼ï¼‰è·³è¿‡å·²ç»LLMå¤„ç†è¿‡çš„è®ºæ–‡'
        )
        parser.add_argument(
            '--max-chars',
            type=int,
            default=50000,
            help='ï¼ˆprocessæ¨¡å¼ï¼‰æœ€å¤§å­—ç¬¦æ•°é™åˆ¶ï¼ˆé»˜è®¤: 50000ï¼‰'
        )
        parser.add_argument(
            '--clean-old-logs',
            action='store_true',
            help='é‡è·‘æ—¶æ¸…ç†è¯¥è®ºæ–‡ä¹‹å‰çš„æ‰€æœ‰æ—¥å¿—è®°å½•ï¼ˆé¿å…é‡å¤ï¼‰'
        )
        parser.add_argument(
            '--workers',
            type=int,
            default=1,
            help='å¹¶å‘å¤„ç†çš„çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 1ï¼Œå»ºè®® extract æ¨¡å¼ä½¿ç”¨ 3-5ï¼Œprocess æ¨¡å¼ä¿æŒ 1ï¼‰'
        )
    
    def handle(self, *args, **options):
        mode = options['mode']
        
        # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒçš„æ ‡é¢˜
        mode_titles = {
            'extract': 'ã€ç¬¬ä¸€é˜¶æ®µã€‘å¼€å§‹æå–ArXivè®ºæ–‡å‚è€ƒæ–‡çŒ®åŸå§‹æ–‡æœ¬...',
            'process': 'ã€ç¬¬äºŒé˜¶æ®µã€‘å¼€å§‹ä½¿ç”¨LLMå¤„ç†å‚è€ƒæ–‡çŒ®...',
            'full': 'å¼€å§‹æå–ArXivè®ºæ–‡å‚è€ƒæ–‡çŒ®ï¼ˆå®Œæ•´æµç¨‹ï¼‰...'
        }
        self.stdout.write(self.style.SUCCESS(mode_titles[mode]))
        
        # åˆå§‹åŒ–æå–å™¨
        try:
            if mode == 'extract':
                # extract æ¨¡å¼ä¸éœ€è¦ LLM é…ç½®
                extractor = ArxivReferenceExtractor(
                    max_retries=options['max_retries']
                )
                self.stdout.write('æå–å™¨å·²åˆå§‹åŒ–ï¼ˆæ— éœ€LLMé…ç½®ï¼‰')
            else:
                # process å’Œ full æ¨¡å¼éœ€è¦ LLM
                extractor = ArxivReferenceExtractor(
                    llm_provider=options['llm_provider'],
                    llm_model=options['llm_model'],
                    max_retries=options['max_retries'],
                    llm_timeout=options['llm_timeout']
                )
                self.stdout.write(f'LLMé…ç½®: {options["llm_provider"]}/{options["llm_model"]} (è¶…æ—¶: {options["llm_timeout"]}ç§’)')
        except Exception as e:
            raise CommandError(f'åˆå§‹åŒ–æå–å™¨å¤±è´¥: {str(e)}')
        
        # å¦‚æœæŒ‡å®šäº†æ¸…ç†PDF
        if options['cleanup_pdfs'] is not None:
            self.stdout.write('æ¸…ç†æ—§çš„PDFæ–‡ä»¶...')
            deleted = extractor.cleanup_old_pdfs(days=options['cleanup_pdfs'])
            self.stdout.write(self.style.SUCCESS(f'æ¸…ç†äº† {deleted} ä¸ªPDFæ–‡ä»¶'))
            if not options['arxiv_id']:
                return
        
        # æ ¹æ®æ¨¡å¼è°ƒç”¨ä¸åŒçš„å¤„ç†æ–¹æ³•
        if mode == 'extract':
            self._handle_extract_mode(extractor, options)
        elif mode == 'process':
            self._handle_process_mode(extractor, options)
        else:  # full
            self._handle_full_mode(extractor, options)
    
    def _get_papers_to_process(self, options):
        """è·å–éœ€è¦å¤„ç†çš„è®ºæ–‡åˆ—è¡¨"""
        queryset = ArxivPaper.objects.all()
        
        # å¦‚æœæŒ‡å®šäº†arxiv_id
        if options['arxiv_id']:
            queryset = queryset.filter(arxiv_id=options['arxiv_id'])
        
        # å¦‚æœè·³è¿‡å·²å¤„ç†çš„
        if options['skip_existing']:
            # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨ LEFT JOIN + IS NULL æ›¿ä»£ exclude(id__in=...)
            # è¿™æ ·å¯ä»¥é¿å…åœ¨ç™¾ä¸‡çº§æ•°æ®ä¸‹åŠ è½½å¤§é‡ ID åˆ°å†…å­˜
            from django.db.models import OuterRef, Exists
            
            # åˆ›å»ºå­æŸ¥è¯¢ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²å®Œæˆçš„æ—¥å¿—
            completed_logs = ArxivReferenceExtractLog.objects.filter(
                paper_id=OuterRef('id'),
                status='completed'
            )
            
            # åªé€‰æ‹©æ²¡æœ‰å·²å®Œæˆæ—¥å¿—çš„è®ºæ–‡
            queryset = queryset.filter(~Exists(completed_logs))
        
        # å¦‚æœåªé‡è¯•å¤±è´¥çš„
        if options['retry_failed']:
            from django.db.models import OuterRef, Exists
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨ Exists å­æŸ¥è¯¢
            failed_logs = ArxivReferenceExtractLog.objects.filter(
                paper_id=OuterRef('id'),
                status='failed'
            )
            
            queryset = queryset.filter(Exists(failed_logs))
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
        queryset = queryset.order_by('-published')
        
        # åº”ç”¨é™åˆ¶
        if options['limit']:
            queryset = queryset[:options['limit']]
        
        return list(queryset)
    
    def _process_single_paper(self, paper, extractor, options):
        """å¤„ç†å•ç¯‡è®ºæ–‡"""
        arxiv_id = paper.arxiv_id
        
        self.stdout.write(f'\n{"="*70}')
        self.stdout.write(f'å¤„ç†è®ºæ–‡: {arxiv_id}')
        self.stdout.write(f'æ ‡é¢˜: {paper.title[:60]}...')
        self.stdout.write(f'{"="*70}')
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æˆåŠŸçš„æå–è®°å½•
        if options['skip_existing']:
            existing_log = ArxivReferenceExtractLog.objects.filter(
                paper=paper,
                status='completed'
            ).first()
            
            if existing_log:
                self.stdout.write(self.style.WARNING(f'  âŠ™ è·³è¿‡ï¼ˆå·²å¤„ç†ï¼‰'))
                return 'skipped'
        
        # å¦‚æœæŒ‡å®šäº†æ¸…ç†æ—§æ—¥å¿—ï¼Œåˆ™åˆ é™¤è¯¥è®ºæ–‡ä¹‹å‰çš„æ‰€æœ‰æ—¥å¿—
        if options['clean_old_logs']:
            old_logs_count = ArxivReferenceExtractLog.objects.filter(paper=paper).count()
            if old_logs_count > 0:
                ArxivReferenceExtractLog.objects.filter(paper=paper).delete()
                self.stdout.write(f'  ğŸ—‘ï¸  æ¸…ç†äº† {old_logs_count} æ¡æ—§æ—¥å¿—è®°å½•')
        
        # æ£€æŸ¥é‡è¯•æ¬¡æ•°
        retry_count = ArxivReferenceExtractLog.objects.filter(
            paper=paper
        ).count()
        
        if retry_count >= options['max_retries'] and not options['retry_failed']:
            self.stdout.write(self.style.WARNING(f'  âŠ™ è·³è¿‡ï¼ˆè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰'))
            return 'skipped'
        
        # åˆ›å»ºæå–æ—¥å¿—
        log = ArxivReferenceExtractLog.objects.create(
            paper=paper,
            status='pending',
            retry_count=retry_count
        )
        
        start_time = timezone.now()
        
        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(step_name, details):
            """æ‰“å°å¤„ç†è¿›åº¦"""
            step_icons = {
                'downloading': 'ğŸ“¥',
                'downloaded': 'âœ…',
                'download_failed': 'âŒ',
                'extracting_text': 'ğŸ“',
                'text_extracted': 'âœ…',
                'extraction_failed': 'âŒ',
                'finding_references': 'ğŸ”',
                'references_found': 'âœ…',
                'reference_not_found': 'âš ï¸',
                'llm_processing': 'ğŸ¤–',
                'llm_completed': 'âœ…',
                'llm_failed': 'âŒ',
            }
            icon = step_icons.get(step_name, 'â€¢')
            self.stdout.write(f'  {icon} {details}')
        
        try:
            # æ‰§è¡Œæå–
            result = extractor.process_paper(
                paper_id=paper.id,
                arxiv_id=arxiv_id,
                pdf_url=paper.pdf_url,
                progress_callback=progress_callback
            )
            
            # æ›´æ–°æ—¥å¿—
            log.pdf_downloaded = result['pdf_downloaded']
            log.pdf_file_path = result['pdf_path']
            log.pdf_file_size = result['pdf_size']
            log.text_extracted = result['text_extracted']
            log.reference_section_found = result['reference_section_found']
            log.llm_processed = result['llm_processed']
            log.reference_count = result['reference_count']
            log.llm_response = result.get('llm_response')  # ä¿å­˜LLMåŸå§‹å“åº”
            
            if result['success']:
                log.status = 'completed'
                
                # ä¿å­˜å‚è€ƒæ–‡çŒ®åˆ°æ•°æ®åº“
                self.stdout.write('  ğŸ’¾ æ­£åœ¨ä¿å­˜å‚è€ƒæ–‡çŒ®åˆ°æ•°æ®åº“...')
                self._save_references(paper, result['references'], log)
                
                log.completed_at = timezone.now()
                log.duration_seconds = (log.completed_at - start_time).seconds
                log.save()
                
                self.stdout.write(self.style.SUCCESS(
                    f'  âœ“ å¤„ç†å®Œæˆï¼æå– {result["reference_count"]} æ¡å‚è€ƒæ–‡çŒ®ï¼Œè€—æ—¶ {log.duration_seconds} ç§’'
                ))
                return 'success'
            else:
                log.status = 'failed'
                log.error_type = result['error_type']
                log.error_message = result['error_message']
                log.completed_at = timezone.now()
                log.duration_seconds = (log.completed_at - start_time).seconds
                log.save()
                
                self.stdout.write(self.style.ERROR(
                    f'  âœ— å¤„ç†å¤±è´¥: {result["error_type"]}'
                ))
                self.stdout.write(self.style.ERROR(
                    f'     é”™è¯¯è¯¦æƒ…: {result["error_message"][:200]}'
                ))
                return 'failed'
                
        except Exception as e:
            log.status = 'failed'
            log.error_type = 'unexpected_error'
            log.error_message = str(e)
            log.completed_at = timezone.now()
            log.duration_seconds = (log.completed_at - start_time).seconds
            log.save()
            
            self.stdout.write(self.style.ERROR(f'  âœ— å¼‚å¸¸: {str(e)}'))
            logger.exception(f'å¤„ç†è®ºæ–‡ {arxiv_id} æ—¶å‘ç”Ÿå¼‚å¸¸')
            return 'failed'
    
    def _save_references(self, paper, references, log):
        """ä¿å­˜å‚è€ƒæ–‡çŒ®åˆ°æ•°æ®åº“"""
        try:
            with transaction.atomic():
                # åˆ é™¤æ—§çš„å‚è€ƒæ–‡çŒ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                ArxivPaperReference.objects.filter(paper=paper).delete()
                
                # æ‰¹é‡åˆ›å»ºæ–°çš„å‚è€ƒæ–‡çŒ®
                reference_objects = []
                for ref_data in references:
                    reference_objects.append(
                        ArxivPaperReference(
                            paper=paper,
                            reference_number=ref_data.get('reference_number', 0),
                            title=ref_data.get('title'),
                            authors=ref_data.get('authors'),
                            year=ref_data.get('year'),
                            venue=ref_data.get('venue'),
                            venue_type=ref_data.get('venue_type'),
                            volume=ref_data.get('volume'),
                            issue=ref_data.get('issue'),
                            pages=ref_data.get('pages'),
                            doi=ref_data.get('doi'),
                            arxiv_id=ref_data.get('arxiv_id'),
                            url=ref_data.get('url'),
                            raw_text=ref_data.get('raw_text', ''),
                            extraction_method='llm',
                        )
                    )
                
                ArxivPaperReference.objects.bulk_create(reference_objects)
                
        except Exception as e:
            log.error_message = f'ä¿å­˜å‚è€ƒæ–‡çŒ®å¤±è´¥: {str(e)}'
            log.save()
            raise
    
    def _print_progress(self, stats):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        self.stdout.write(
            f"\nè¿›åº¦: {stats['processed']}/{stats['total']} | "
            f"æˆåŠŸ: {stats['success']} | "
            f"å¤±è´¥: {stats['failed']} | "
            f"è·³è¿‡: {stats['skipped']}"
        )
    
    def _print_final_stats(self, stats):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        # è®¡ç®—å‚è€ƒæ–‡çŒ®æ€»æ•°
        total_refs = ArxivPaperReference.objects.count()
        
        self.stdout.write(f'æ€»è®ºæ–‡æ•°: {stats["total"]}')
        self.stdout.write(self.style.SUCCESS(f'æˆåŠŸ: {stats["success"]}'))
        if stats['failed'] > 0:
            self.stdout.write(self.style.ERROR(f'å¤±è´¥: {stats["failed"]}'))
        if stats['skipped'] > 0:
            self.stdout.write(self.style.WARNING(f'è·³è¿‡: {stats["skipped"]}'))
        self.stdout.write(f'æ•°æ®åº“ä¸­å‚è€ƒæ–‡çŒ®æ€»æ•°: {total_refs}')
        
        # æˆåŠŸç‡
        if stats['processed'] > 0:
            success_rate = (stats['success'] / stats['processed']) * 100
            self.stdout.write(f'æˆåŠŸç‡: {success_rate:.1f}%')
    
    def _handle_extract_mode(self, extractor, options):
        """å¤„ç† extract æ¨¡å¼ï¼šåªæå–å‚è€ƒæ–‡çŒ®åŸå§‹æ–‡æœ¬"""
        # è·å–éœ€è¦å¤„ç†çš„è®ºæ–‡
        papers = self._get_papers_to_process(options)
        
        if not papers:
            self.stdout.write(self.style.WARNING('æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„è®ºæ–‡'))
            return
        
        total = len(papers)
        workers = options['workers']
        self.stdout.write(f'æ‰¾åˆ° {total} ç¯‡è®ºæ–‡å¾…å¤„ç†')
        
        if workers > 1:
            self.stdout.write(f'ä½¿ç”¨ {workers} ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†')
        
        # çº¿ç¨‹å®‰å…¨çš„ç»Ÿè®¡ä¿¡æ¯
        stats = ThreadSafeStats(total)
        
        # å¦‚æœåªæœ‰ 1 ä¸ª workerï¼Œä½¿ç”¨å•çº¿ç¨‹å¤„ç†
        if workers == 1:
            self._handle_extract_mode_single_thread(papers, extractor, options, stats)
        else:
            self._handle_extract_mode_multi_thread(papers, extractor, options, stats, workers)
        
        # æœ€ç»ˆç»Ÿè®¡
        self.stdout.write('\n' + '=' * 60)
        self.stdout.write(self.style.SUCCESS('ã€ç¬¬ä¸€é˜¶æ®µã€‘å¤„ç†å®Œæˆï¼'))
        self._print_extract_final_stats(stats.get_dict())
    
    def _handle_extract_mode_single_thread(self, papers, extractor, options, stats):
        """å•çº¿ç¨‹å¤„ç† extract æ¨¡å¼"""
        batch_size = options['batch_size']
        delay = options['delay']
        total = len(papers)
        
        for i in range(0, total, batch_size):
            batch = papers[i:i + batch_size]
            self.stdout.write(f'\nå¤„ç†æ‰¹æ¬¡ {i // batch_size + 1} (è®ºæ–‡ {i + 1}-{min(i + batch_size, total)})')
            
            for paper in batch:
                result = self._process_paper_extract_only(paper, extractor, options)
                stats.update(result)
                
                # å»¶è¿Ÿ
                if delay > 0 and stats.processed < total:
                    time.sleep(delay)
            
            # æ˜¾ç¤ºè¿›åº¦
            self._print_progress(stats.get_dict())
    
    def _handle_extract_mode_multi_thread(self, papers, extractor, options, stats, workers):
        """å¤šçº¿ç¨‹å¤„ç† extract æ¨¡å¼"""
        delay = options['delay']
        total = len(papers)
        output_lock = threading.Lock()
        
        def process_paper_with_output(paper):
            """\u5904\u7406\u5355\u7bc7\u8bba\u6587\uff08\u5e26\u8f93\u51fa\u9501\uff09"""
            result = self._process_paper_extract_only(paper, extractor, options)
            stats.update(result)
            
            # çº¿ç¨‹å®‰å…¨åœ°æ‰“å°è¿›åº¦
            with output_lock:
                if stats.processed % 10 == 0 or stats.processed == total:
                    self._print_progress(stats.get_dict())
            
            # å»¶è¿Ÿ
            if delay > 0:
                time.sleep(delay)
            
            return result
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†
        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = {executor.submit(process_paper_with_output, paper): paper for paper in papers}
            
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    paper = futures[future]
                    with output_lock:
                        self.stdout.write(self.style.ERROR(f'å¤„ç†è®ºæ–‡ {paper.arxiv_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}'))
                        logger.exception(f'å¤šçº¿ç¨‹å¤„ç†è®ºæ–‡ {paper.arxiv_id} å¤±è´¥')
    
    def _handle_process_mode(self, extractor, options):
        """å¤„ç† process æ¨¡å¼ï¼šåªä½¿ç”¨LLMå¤„ç†å·²æå–çš„æ–‡æœ¬"""
        # è·å–éœ€è¦å¤„ç†çš„æ—¥å¿—è®°å½•
        logs = self._get_logs_to_process(options)
        
        if not logs:
            self.stdout.write(self.style.WARNING('æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„è®°å½•'))
            return
        
        total = len(logs)
        self.stdout.write(f'æ‰¾åˆ° {total} æ¡è®°å½•å¾…å¤„ç†')
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total': total,
            'processed': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0,
        }
        
        # æ‰¹é‡å¤„ç†
        batch_size = options['batch_size']
        delay = options['delay']
        
        for i in range(0, total, batch_size):
            batch = logs[i:i + batch_size]
            self.stdout.write(f'\nå¤„ç†æ‰¹æ¬¡ {i // batch_size + 1} (è®°å½• {i + 1}-{min(i + batch_size, total)})')
            
            for log in batch:
                result = self._process_log_with_llm(log, extractor, options)
                
                # æ›´æ–°ç»Ÿè®¡
                stats['processed'] += 1
                if result == 'success':
                    stats['success'] += 1
                elif result == 'failed':
                    stats['failed'] += 1
                elif result == 'skipped':
                    stats['skipped'] += 1
                
                # å»¶è¿Ÿ
                if delay > 0 and stats['processed'] < total:
                    time.sleep(delay)
            
            # æ˜¾ç¤ºè¿›åº¦
            self._print_progress(stats)
        
        # æœ€ç»ˆç»Ÿè®¡
        self.stdout.write('\n' + '=' * 60)
        self.stdout.write(self.style.SUCCESS('ã€ç¬¬äºŒé˜¶æ®µã€‘å¤„ç†å®Œæˆï¼'))
        self._print_process_final_stats(stats)
    
    def _handle_full_mode(self, extractor, options):
        """å¤„ç† full æ¨¡å¼ï¼šå®Œæ•´æµç¨‹ï¼ˆæå–æ–‡æœ¬ + LLMå¤„ç†ï¼‰"""
        # è·å–éœ€è¦å¤„ç†çš„è®ºæ–‡
        papers = self._get_papers_to_process(options)
        
        if not papers:
            self.stdout.write(self.style.WARNING('æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„è®ºæ–‡'))
            return
        
        total = len(papers)
        workers = options['workers']
        self.stdout.write(f'æ‰¾åˆ° {total} ç¯‡è®ºæ–‡å¾…å¤„ç†')
        
        if workers > 1:
            self.stdout.write(f'ä½¿ç”¨ {workers} ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†')
            self.stdout.write(self.style.WARNING('æ³¨æ„: full æ¨¡å¼åŒ…å« LLM è°ƒç”¨ï¼Œå»ºè®® workers è®¾ç½®ä¸º 2-3 ä»¥é¿å… API é™æµ'))
        
        # çº¿ç¨‹å®‰å…¨çš„ç»Ÿè®¡ä¿¡æ¯
        stats = ThreadSafeStats(total)
        
        # å¦‚æœåªæœ‰ 1 ä¸ª workerï¼Œä½¿ç”¨å•çº¿ç¨‹å¤„ç†
        if workers == 1:
            self._handle_full_mode_single_thread(papers, extractor, options, stats)
        else:
            self._handle_full_mode_multi_thread(papers, extractor, options, stats, workers)
        
        # æœ€ç»ˆç»Ÿè®¡
        self.stdout.write('\n' + '=' * 60)
        self.stdout.write(self.style.SUCCESS('å¤„ç†å®Œæˆï¼'))
        self._print_final_stats(stats.get_dict())
    
    def _handle_full_mode_single_thread(self, papers, extractor, options, stats):
        """å•çº¿ç¨‹å¤„ç† full æ¨¡å¼"""
        batch_size = options['batch_size']
        delay = options['delay']
        total = len(papers)
        
        for i in range(0, total, batch_size):
            batch = papers[i:i + batch_size]
            self.stdout.write(f'\nå¤„ç†æ‰¹æ¬¡ {i // batch_size + 1} (è®ºæ–‡ {i + 1}-{min(i + batch_size, total)})')
            
            for paper in batch:
                result = self._process_single_paper(paper, extractor, options)
                stats.update(result)
                
                # å»¶è¿Ÿ
                if delay > 0 and stats.processed < total:
                    time.sleep(delay)
            
            # æ˜¾ç¤ºè¿›åº¦
            self._print_progress(stats.get_dict())
    
    def _handle_full_mode_multi_thread(self, papers, extractor, options, stats, workers):
        """å¤šçº¿ç¨‹å¤„ç† full æ¨¡å¼"""
        delay = options['delay']
        total = len(papers)
        output_lock = threading.Lock()
        
        def process_paper_with_output(paper):
            """\u5904\u7406\u5355\u7bc7\u8bba\u6587\uff08\u5e26\u8f93\u51fa\u9501\uff09"""
            result = self._process_single_paper(paper, extractor, options)
            stats.update(result)
            
            # çº¿ç¨‹å®‰å…¨åœ°æ‰“å°è¿›åº¦
            with output_lock:
                if stats.processed % 5 == 0 or stats.processed == total:
                    self._print_progress(stats.get_dict())
            
            # å»¶è¿Ÿ
            if delay > 0:
                time.sleep(delay)
            
            return result
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†
        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = {executor.submit(process_paper_with_output, paper): paper for paper in papers}
            
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    paper = futures[future]
                    with output_lock:
                        self.stdout.write(self.style.ERROR(f'å¤„ç†è®ºæ–‡ {paper.arxiv_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}'))
                        logger.exception(f'å¤šçº¿ç¨‹å¤„ç†è®ºæ–‡ {paper.arxiv_id} å¤±è´¥')
    
    def _get_logs_to_process(self, options):
        """è·å–éœ€è¦å¤„ç†çš„æå–æ—¥å¿—åˆ—è¡¨ï¼ˆç”¨äºprocessæ¨¡å¼ï¼‰"""
        # åŸºç¡€æŸ¥è¯¢ï¼šåªé€‰æ‹©å·²æå–æ–‡æœ¬ä½†æœªLLMå¤„ç†çš„è®°å½•
        queryset = ArxivReferenceExtractLog.objects.filter(
            reference_section_found=True,
            reference_raw_text__isnull=False
        )
        
        # å¦‚æœæŒ‡å®šäº†arxiv_id
        if options['arxiv_id']:
            queryset = queryset.filter(paper__arxiv_id=options['arxiv_id'])
        
        # å¦‚æœè·³è¿‡å·²å¤„ç†çš„
        if options['skip_processed']:
            queryset = queryset.filter(llm_processed=False)
        
        # å¦‚æœåªé‡è¯•å¤±è´¥çš„
        if options['retry_failed']:
            queryset = queryset.filter(
                llm_processed=False,
                status='failed'
            )
        elif not options['skip_processed']:
            # é»˜è®¤åªå¤„ç†æœªLLMå¤„ç†çš„
            queryset = queryset.filter(llm_processed=False)
        
        # æŒ‰æ—¶é—´æ’åº
        queryset = queryset.select_related('paper').order_by('-started_at')
        
        # åº”ç”¨é™åˆ¶
        if options['limit']:
            queryset = queryset[:options['limit']]
        
        return list(queryset)
    
    def _process_paper_extract_only(self, paper, extractor, options):
        """å¤„ç†å•ç¯‡è®ºæ–‡ï¼ˆåªæå–æ–‡æœ¬ï¼Œä¸è°ƒç”¨LLMï¼‰"""
        arxiv_id = paper.arxiv_id
        
        self.stdout.write(f'\n{"="*70}')
        self.stdout.write(f'å¤„ç†è®ºæ–‡: {arxiv_id}')
        self.stdout.write(f'æ ‡é¢˜: {paper.title[:60]}...')
        self.stdout.write(f'{"="*70}')
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æå–è®°å½•
        if options['skip_existing']:
            existing_log = ArxivReferenceExtractLog.objects.filter(
                paper=paper,
                reference_section_found=True,
                reference_raw_text__isnull=False
            ).first()
            
            if existing_log:
                self.stdout.write(self.style.WARNING(f'  âŠ™ è·³è¿‡ï¼ˆå·²æå–æ–‡æœ¬ï¼‰'))
                return 'skipped'
        
        # å¦‚æœæŒ‡å®šäº†æ¸…ç†æ—§æ—¥å¿—ï¼Œåˆ™åˆ é™¤è¯¥è®ºæ–‡ä¹‹å‰çš„æ‰€æœ‰æ—¥å¿—
        if options['clean_old_logs']:
            old_logs_count = ArxivReferenceExtractLog.objects.filter(paper=paper).count()
            if old_logs_count > 0:
                ArxivReferenceExtractLog.objects.filter(paper=paper).delete()
                self.stdout.write(f'  ğŸ—‘ï¸  æ¸…ç†äº† {old_logs_count} æ¡æ—§æ—¥å¿—è®°å½•')
        
        # æ£€æŸ¥é‡è¯•æ¬¡æ•°
        retry_count = ArxivReferenceExtractLog.objects.filter(
            paper=paper
        ).count()
        
        if retry_count >= options['max_retries'] and not options['retry_failed']:
            self.stdout.write(self.style.WARNING(f'  âŠ™ è·³è¿‡ï¼ˆè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰'))
            return 'skipped'
        
        # åˆ›å»ºæå–æ—¥å¿—
        log = ArxivReferenceExtractLog.objects.create(
            paper=paper,
            status='extracting',
            retry_count=retry_count
        )
        
        start_time = timezone.now()
        
        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(step_name, details):
            """æ‰“å°å¤„ç†è¿›åº¦"""
            step_icons = {
                'downloading': 'ğŸ“¥',
                'downloaded': 'âœ…',
                'download_failed': 'âŒ',
                'extracting_text': 'ğŸ“',
                'text_extracted': 'âœ…',
                'extraction_failed': 'âŒ',
                'finding_references': 'ğŸ”',
                'references_found': 'âœ…',
                'reference_not_found': 'âš ï¸',
            }
            icon = step_icons.get(step_name, 'â€¢')
            self.stdout.write(f'  {icon} {details}')
        
        try:
            # æ‰§è¡Œæå–ï¼ˆåªæå–æ–‡æœ¬ï¼Œä¸è°ƒç”¨LLMï¼‰
            result = extractor.extract_reference_text_only(
                paper_id=paper.id,
                arxiv_id=arxiv_id,
                pdf_url=paper.pdf_url,
                progress_callback=progress_callback
            )
            
            # æ›´æ–°æ—¥å¿—
            log.pdf_downloaded = result['pdf_downloaded']
            log.pdf_file_path = result['pdf_path']
            log.pdf_file_size = result['pdf_size']
            log.text_extracted = result['text_extracted']
            log.reference_section_found = result['reference_section_found']
            
            if result['success']:
                log.status = 'completed'
                log.reference_raw_text = result['reference_text']
                log.reference_text_length = result['reference_text_length']
                log.completed_at = timezone.now()
                log.duration_seconds = (log.completed_at - start_time).seconds
                log.save()
                
                self.stdout.write(self.style.SUCCESS(
                    f'  âœ“ æå–å®Œæˆï¼å‚è€ƒæ–‡çŒ®æ–‡æœ¬é•¿åº¦: {result["reference_text_length"]} å­—ç¬¦ï¼Œè€—æ—¶ {log.duration_seconds} ç§’'
                ))
                return 'success'
            else:
                log.status = 'failed'
                log.error_type = result['error_type']
                log.error_message = result['error_message']
                log.completed_at = timezone.now()
                log.duration_seconds = (log.completed_at - start_time).seconds
                log.save()
                
                self.stdout.write(self.style.ERROR(
                    f'  âœ— æå–å¤±è´¥: {result["error_type"]}'
                ))
                self.stdout.write(self.style.ERROR(
                    f'     é”™è¯¯è¯¦æƒ…: {result["error_message"][:200]}'
                ))
                return 'failed'
                
        except Exception as e:
            log.status = 'failed'
            log.error_type = 'unexpected_error'
            log.error_message = str(e)
            log.completed_at = timezone.now()
            log.duration_seconds = (log.completed_at - start_time).seconds
            log.save()
            
            self.stdout.write(self.style.ERROR(f'  âœ— å¼‚å¸¸: {str(e)}'))
            logger.exception(f'å¤„ç†è®ºæ–‡ {arxiv_id} æ—¶å‘ç”Ÿå¼‚å¸¸')
            return 'failed'
    
    def _process_log_with_llm(self, log, extractor, options):
        """å¤„ç†å•æ¡æå–è®°å½•ï¼ˆåªä½¿ç”¨LLMå¤„ç†ï¼‰"""
        paper = log.paper
        arxiv_id = paper.arxiv_id
        
        self.stdout.write(f'\n{"="*70}')
        self.stdout.write(f'å¤„ç†è®ºæ–‡: {arxiv_id}')
        self.stdout.write(f'æ ‡é¢˜: {paper.title[:60]}...')
        self.stdout.write(f'å‚è€ƒæ–‡çŒ®æ–‡æœ¬é•¿åº¦: {log.reference_text_length} å­—ç¬¦')
        self.stdout.write(f'{"="*70}')
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
        if options['skip_processed'] and log.llm_processed:
            self.stdout.write(self.style.WARNING(f'  âŠ™ è·³è¿‡ï¼ˆå·²LLMå¤„ç†ï¼‰'))
            return 'skipped'
        
        start_time = timezone.now()
        
        try:
            # ä½¿ç”¨LLMå¤„ç†å‚è€ƒæ–‡çŒ®æ–‡æœ¬
            self.stdout.write(f'  ğŸ¤– æ­£åœ¨è°ƒç”¨ {options["llm_provider"]}/{options["llm_model"]} è§£æå‚è€ƒæ–‡çŒ®...')
            
            success, references, error, llm_response = extractor.process_reference_text_with_llm(
                reference_text=log.reference_raw_text,
                arxiv_id=arxiv_id,
                max_chars=options['max_chars']
            )
            
            # ä¿å­˜LLMåŸå§‹å“åº”
            log.llm_response = llm_response
            
            if success:
                log.llm_processed = True
                log.reference_count = len(references)
                log.status = 'completed'
                
                # ä¿å­˜å‚è€ƒæ–‡çŒ®åˆ°æ•°æ®åº“
                self.stdout.write('  ğŸ’¾ æ­£åœ¨ä¿å­˜å‚è€ƒæ–‡çŒ®åˆ°æ•°æ®åº“...')
                self._save_references(paper, references, log)
                
                log.completed_at = timezone.now()
                log.duration_seconds = (log.completed_at - start_time).seconds
                log.save()
                
                self.stdout.write(self.style.SUCCESS(
                    f'  âœ“ å¤„ç†å®Œæˆï¼æå– {log.reference_count} æ¡å‚è€ƒæ–‡çŒ®ï¼Œè€—æ—¶ {log.duration_seconds} ç§’'
                ))
                return 'success'
            else:
                log.llm_processed = False
                log.error_type = 'llm_error'
                log.error_message = error
                log.status = 'failed'
                log.completed_at = timezone.now()
                log.duration_seconds = (log.completed_at - start_time).seconds
                log.save()
                
                self.stdout.write(self.style.ERROR(
                    f'  âœ— LLMå¤„ç†å¤±è´¥'
                ))
                self.stdout.write(self.style.ERROR(
                    f'     é”™è¯¯è¯¦æƒ…: {error[:200]}'
                ))
                return 'failed'
                
        except Exception as e:
            log.llm_processed = False
            log.error_type = 'unexpected_error'
            log.error_message = str(e)
            log.status = 'failed'
            log.completed_at = timezone.now()
            log.duration_seconds = (log.completed_at - start_time).seconds
            log.save()
            
            self.stdout.write(self.style.ERROR(f'  âœ— å¼‚å¸¸: {str(e)}'))
            logger.exception(f'å¤„ç†è®ºæ–‡ {arxiv_id} æ—¶å‘ç”Ÿå¼‚å¸¸')
            return 'failed'
    
    def _print_extract_final_stats(self, stats):
        """æ‰“å°extractæ¨¡å¼çš„æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        # è®¡ç®—å·²æå–æ–‡æœ¬çš„è®°å½•æ•°
        total_extracted = ArxivReferenceExtractLog.objects.filter(
            reference_section_found=True,
            reference_raw_text__isnull=False
        ).count()
        
        self.stdout.write(f'æ€»è®ºæ–‡æ•°: {stats["total"]}')
        self.stdout.write(self.style.SUCCESS(f'æˆåŠŸ: {stats["success"]}'))
        if stats['failed'] > 0:
            self.stdout.write(self.style.ERROR(f'å¤±è´¥: {stats["failed"]}'))
        if stats['skipped'] > 0:
            self.stdout.write(self.style.WARNING(f'è·³è¿‡: {stats["skipped"]}'))
        self.stdout.write(f'æ•°æ®åº“ä¸­å·²æå–å‚è€ƒæ–‡çŒ®æ–‡æœ¬çš„è®°å½•æ•°: {total_extracted}')
        
        # æˆåŠŸç‡
        if stats['processed'] > 0:
            success_rate = (stats['success'] / stats['processed']) * 100
            self.stdout.write(f'æˆåŠŸç‡: {success_rate:.1f}%')
    
    def _print_process_final_stats(self, stats):
        """æ‰“å°processæ¨¡å¼çš„æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        # è®¡ç®—å‚è€ƒæ–‡çŒ®æ€»æ•°
        total_refs = ArxivPaperReference.objects.count()
        
        # è®¡ç®—å·²LLMå¤„ç†çš„è®°å½•æ•°
        total_llm_processed = ArxivReferenceExtractLog.objects.filter(
            llm_processed=True
        ).count()
        
        self.stdout.write(f'æ€»è®°å½•æ•°: {stats["total"]}')
        self.stdout.write(self.style.SUCCESS(f'æˆåŠŸ: {stats["success"]}'))
        if stats['failed'] > 0:
            self.stdout.write(self.style.ERROR(f'å¤±è´¥: {stats["failed"]}'))
        if stats['skipped'] > 0:
            self.stdout.write(self.style.WARNING(f'è·³è¿‡: {stats["skipped"]}'))
        self.stdout.write(f'æ•°æ®åº“ä¸­å‚è€ƒæ–‡çŒ®æ€»æ•°: {total_refs}')
        self.stdout.write(f'æ•°æ®åº“ä¸­å·²LLMå¤„ç†çš„è®°å½•æ•°: {total_llm_processed}')
        
        # æˆåŠŸç‡
        if stats['processed'] > 0:
            success_rate = (stats['success'] / stats['processed']) * 100
            self.stdout.write(f'æˆåŠŸç‡: {success_rate:.1f}%')
