"""
Djangoç®¡ç†å‘½ä»¤ï¼šç¬¬äºŒé˜¶æ®µ - ä½¿ç”¨LLMå¤„ç†å·²æå–çš„å‚è€ƒæ–‡çŒ®æ–‡æœ¬
ä»æ•°æ®åº“è¯»å–å·²æå–çš„å‚è€ƒæ–‡çŒ®åŸå§‹æ–‡æœ¬ï¼Œä½¿ç”¨LLMè¿›è¡Œç»“æ„åŒ–å¤„ç†
"""
from django.core.management.base import BaseCommand, CommandError
from django.db import transaction
from django.utils import timezone
from datetime import datetime, timedelta
import logging
import time

from core.arxiv_models import ArxivPaper, ArxivPaperReference, ArxivReferenceExtractLog
from core.arxiv_reference_extractor import ArxivReferenceExtractor


# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = 'ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨LLMå¤„ç†å·²æå–çš„å‚è€ƒæ–‡çŒ®åŸå§‹æ–‡æœ¬'
    
    def add_arguments(self, parser):
        parser.add_argument(
            '--arxiv-id',
            type=str,
            help='å¤„ç†æŒ‡å®šçš„arXiv ID'
        )
        parser.add_argument(
            '--limit',
            type=int,
            default=None,
            help='é™åˆ¶å¤„ç†çš„è®ºæ–‡æ•°é‡'
        )
        parser.add_argument(
            '--batch-size',
            type=int,
            default=5,
            help='æ‰¹é‡å¤„ç†çš„å¤§å°ï¼ˆç”±äºè°ƒç”¨LLMï¼Œå»ºè®®è®¾ç½®è¾ƒå°å€¼ï¼‰'
        )
        parser.add_argument(
            '--skip-processed',
            action='store_true',
            help='è·³è¿‡å·²ç»LLMå¤„ç†è¿‡çš„è®ºæ–‡'
        )
        parser.add_argument(
            '--retry-failed',
            action='store_true',
            help='é‡è¯•ä¹‹å‰LLMå¤„ç†å¤±è´¥çš„è®ºæ–‡'
        )
        parser.add_argument(
            '--llm-provider',
            type=str,
            default='qwen',
            help='LLMæä¾›å•†ï¼ˆé»˜è®¤: qwenï¼‰'
        )
        parser.add_argument(
            '--llm-model',
            type=str,
            default='qwen-max',
            help='LLMæ¨¡å‹åç§°ï¼ˆé»˜è®¤: qwen-maxï¼‰'
        )
        parser.add_argument(
            '--llm-timeout',
            type=int,
            default=600,
            help='LLM APIè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤: 600ï¼‰'
        )
        parser.add_argument(
            '--delay',
            type=float,
            default=2.0,
            help='æ¯ç¯‡è®ºæ–‡å¤„ç†ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰'
        )
        parser.add_argument(
            '--max-chars',
            type=int,
            default=50000,
            help='æœ€å¤§å­—ç¬¦æ•°é™åˆ¶ï¼ˆé»˜è®¤: 50000ï¼‰'
        )
    
    def handle(self, *args, **options):
        self.stdout.write(self.style.SUCCESS('ã€ç¬¬äºŒé˜¶æ®µã€‘å¼€å§‹ä½¿ç”¨LLMå¤„ç†å‚è€ƒæ–‡çŒ®...'))
        
        # åˆå§‹åŒ–æå–å™¨
        try:
            extractor = ArxivReferenceExtractor(
                llm_provider=options['llm_provider'],
                llm_model=options['llm_model'],
                llm_timeout=options['llm_timeout']
            )
            self.stdout.write(f'LLMé…ç½®: {options["llm_provider"]}/{options["llm_model"]} (è¶…æ—¶: {options["llm_timeout"]}ç§’)')
        except Exception as e:
            raise CommandError(f'åˆå§‹åŒ–æå–å™¨å¤±è´¥: {str(e)}')
        
        # è·å–éœ€è¦å¤„ç†çš„è®°å½•
        logs = self._get_logs_to_process(options)
        
        if not logs:
            self.stdout.write(self.style.WARNING('æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„è®°å½•'))
            return
        
        total = len(logs)
        self.stdout.write(f'æ‰¾åˆ° {total} æ¡è®°å½•å¾…å¤„ç†')
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total': total,
            'processed': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'total_references': 0,
        }
        
        # æ‰¹é‡å¤„ç†
        batch_size = options['batch_size']
        delay = options['delay']
        
        for i in range(0, total, batch_size):
            batch = logs[i:i + batch_size]
            self.stdout.write(f'\nå¤„ç†æ‰¹æ¬¡ {i // batch_size + 1} (è®°å½• {i + 1}-{min(i + batch_size, total)})')
            
            for log in batch:
                result = self._process_single_log(log, extractor, options)
                
                # æ›´æ–°ç»Ÿè®¡
                stats['processed'] += 1
                if result == 'success':
                    stats['success'] += 1
                elif result == 'failed':
                    stats['failed'] += 1
                elif result == 'skipped':
                    stats['skipped'] += 1
                
                # å»¶è¿Ÿ
                if delay > 0 and stats['processed'] < total:
                    time.sleep(delay)
            
            # æ˜¾ç¤ºè¿›åº¦
            self._print_progress(stats)
        
        # æœ€ç»ˆç»Ÿè®¡
        self.stdout.write('\n' + '=' * 60)
        self.stdout.write(self.style.SUCCESS('ã€ç¬¬äºŒé˜¶æ®µã€‘å¤„ç†å®Œæˆï¼'))
        self._print_final_stats(stats)
    
    def _get_logs_to_process(self, options):
        """è·å–éœ€è¦å¤„ç†çš„æå–æ—¥å¿—åˆ—è¡¨"""
        # åŸºç¡€æŸ¥è¯¢ï¼šåªé€‰æ‹©å·²æå–æ–‡æœ¬ä½†æœªLLMå¤„ç†çš„è®°å½•
        queryset = ArxivReferenceExtractLog.objects.filter(
            reference_section_found=True,
            reference_raw_text__isnull=False
        )
        
        # å¦‚æœæŒ‡å®šäº†arxiv_id
        if options['arxiv_id']:
            queryset = queryset.filter(paper__arxiv_id=options['arxiv_id'])
        
        # å¦‚æœè·³è¿‡å·²å¤„ç†çš„
        if options['skip_processed']:
            queryset = queryset.filter(llm_processed=False)
        
        # å¦‚æœåªé‡è¯•å¤±è´¥çš„
        if options['retry_failed']:
            queryset = queryset.filter(
                llm_processed=False,
                status='failed'
            )
        elif not options['skip_processed']:
            # é»˜è®¤åªå¤„ç†æœªLLMå¤„ç†çš„
            queryset = queryset.filter(llm_processed=False)
        
        # æŒ‰æ—¶é—´æ’åº
        queryset = queryset.select_related('paper').order_by('-started_at')
        
        # åº”ç”¨é™åˆ¶
        if options['limit']:
            queryset = queryset[:options['limit']]
        
        return list(queryset)
    
    def _process_single_log(self, log, extractor, options):
        """å¤„ç†å•æ¡æå–è®°å½•"""
        paper = log.paper
        arxiv_id = paper.arxiv_id
        
        self.stdout.write(f'\n{"="*70}')
        self.stdout.write(f'å¤„ç†è®ºæ–‡: {arxiv_id}')
        self.stdout.write(f'æ ‡é¢˜: {paper.title[:60]}...')
        self.stdout.write(f'å‚è€ƒæ–‡çŒ®æ–‡æœ¬é•¿åº¦: {log.reference_text_length} å­—ç¬¦')
        self.stdout.write(f'{"="*70}')
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
        if options['skip_processed'] and log.llm_processed:
            self.stdout.write(self.style.WARNING(f'  âŠ™ è·³è¿‡ï¼ˆå·²LLMå¤„ç†ï¼‰'))
            return 'skipped'
        
        start_time = timezone.now()
        
        try:
            # ä½¿ç”¨LLMå¤„ç†å‚è€ƒæ–‡çŒ®æ–‡æœ¬
            self.stdout.write(f'  ğŸ¤– æ­£åœ¨è°ƒç”¨ {options["llm_provider"]}/{options["llm_model"]} è§£æå‚è€ƒæ–‡çŒ®...')
            
            success, references, error, llm_response = extractor.process_reference_text_with_llm(
                reference_text=log.reference_raw_text,
                arxiv_id=arxiv_id,
                max_chars=options['max_chars']
            )
            
            # ä¿å­˜LLMåŸå§‹å“åº”
            log.llm_response = llm_response
            
            if success:
                log.llm_processed = True
                log.reference_count = len(references)
                log.status = 'completed'
                
                # ä¿å­˜å‚è€ƒæ–‡çŒ®åˆ°æ•°æ®åº“
                self.stdout.write('  ğŸ’¾ æ­£åœ¨ä¿å­˜å‚è€ƒæ–‡çŒ®åˆ°æ•°æ®åº“...')
                self._save_references(paper, references, log)
                
                log.completed_at = timezone.now()
                log.duration_seconds = (log.completed_at - start_time).seconds
                log.save()
                
                self.stdout.write(self.style.SUCCESS(
                    f'  âœ“ å¤„ç†å®Œæˆï¼æå– {log.reference_count} æ¡å‚è€ƒæ–‡çŒ®ï¼Œè€—æ—¶ {log.duration_seconds} ç§’'
                ))
                return 'success'
            else:
                log.llm_processed = False
                log.error_type = 'llm_error'
                log.error_message = error
                log.status = 'failed'
                log.completed_at = timezone.now()
                log.duration_seconds = (log.completed_at - start_time).seconds
                log.save()
                
                self.stdout.write(self.style.ERROR(
                    f'  âœ— LLMå¤„ç†å¤±è´¥'
                ))
                self.stdout.write(self.style.ERROR(
                    f'     é”™è¯¯è¯¦æƒ…: {error[:200]}'
                ))
                return 'failed'
                
        except Exception as e:
            log.llm_processed = False
            log.error_type = 'unexpected_error'
            log.error_message = str(e)
            log.status = 'failed'
            log.completed_at = timezone.now()
            log.duration_seconds = (log.completed_at - start_time).seconds
            log.save()
            
            self.stdout.write(self.style.ERROR(f'  âœ— å¼‚å¸¸: {str(e)}'))
            logger.exception(f'å¤„ç†è®ºæ–‡ {arxiv_id} æ—¶å‘ç”Ÿå¼‚å¸¸')
            return 'failed'
    
    def _save_references(self, paper, references, log):
        """ä¿å­˜å‚è€ƒæ–‡çŒ®åˆ°æ•°æ®åº“"""
        try:
            with transaction.atomic():
                # åˆ é™¤æ—§çš„å‚è€ƒæ–‡çŒ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                ArxivPaperReference.objects.filter(paper=paper).delete()
                
                # æ‰¹é‡åˆ›å»ºæ–°çš„å‚è€ƒæ–‡çŒ®
                reference_objects = []
                for ref_data in references:
                    reference_objects.append(
                        ArxivPaperReference(
                            paper=paper,
                            reference_number=ref_data.get('reference_number', 0),
                            title=ref_data.get('title'),
                            authors=ref_data.get('authors'),
                            year=ref_data.get('year'),
                            venue=ref_data.get('venue'),
                            venue_type=ref_data.get('venue_type'),
                            volume=ref_data.get('volume'),
                            issue=ref_data.get('issue'),
                            pages=ref_data.get('pages'),
                            doi=ref_data.get('doi'),
                            arxiv_id=ref_data.get('arxiv_id'),
                            url=ref_data.get('url'),
                            raw_text=ref_data.get('raw_text', ''),
                            extraction_method='llm',
                        )
                    )
                
                ArxivPaperReference.objects.bulk_create(reference_objects)
                
        except Exception as e:
            log.error_message = f'ä¿å­˜å‚è€ƒæ–‡çŒ®å¤±è´¥: {str(e)}'
            log.save()
            raise
    
    def _print_progress(self, stats):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        self.stdout.write(
            f"\nè¿›åº¦: {stats['processed']}/{stats['total']} | "
            f"æˆåŠŸ: {stats['success']} | "
            f"å¤±è´¥: {stats['failed']} | "
            f"è·³è¿‡: {stats['skipped']}"
        )
    
    def _print_final_stats(self, stats):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        # è®¡ç®—å‚è€ƒæ–‡çŒ®æ€»æ•°
        total_refs = ArxivPaperReference.objects.count()
        
        # è®¡ç®—å·²LLMå¤„ç†çš„è®°å½•æ•°
        total_llm_processed = ArxivReferenceExtractLog.objects.filter(
            llm_processed=True
        ).count()
        
        self.stdout.write(f'æ€»è®°å½•æ•°: {stats["total"]}')
        self.stdout.write(self.style.SUCCESS(f'æˆåŠŸ: {stats["success"]}'))
        if stats['failed'] > 0:
            self.stdout.write(self.style.ERROR(f'å¤±è´¥: {stats["failed"]}'))
        if stats['skipped'] > 0:
            self.stdout.write(self.style.WARNING(f'è·³è¿‡: {stats["skipped"]}'))
        self.stdout.write(f'æ•°æ®åº“ä¸­å‚è€ƒæ–‡çŒ®æ€»æ•°: {total_refs}')
        self.stdout.write(f'æ•°æ®åº“ä¸­å·²LLMå¤„ç†çš„è®°å½•æ•°: {total_llm_processed}')
        
        # æˆåŠŸç‡
        if stats['processed'] > 0:
            success_rate = (stats['success'] / stats['processed']) * 100
            self.stdout.write(f'æˆåŠŸç‡: {success_rate:.1f}%')
