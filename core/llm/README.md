# LLM å®¢æˆ·ç«¯æ¨¡å—

è¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£æ¨¡å—ï¼Œæ”¯æŒå¤šä¸ª LLM æä¾›å•†ï¼Œä½¿ç”¨å·¥å‚è®¾è®¡æ¨¡å¼ï¼Œé…ç½®é›†ä¸­ç®¡ç†ã€‚

## ç‰¹æ€§

- ğŸ­ **å·¥å‚æ¨¡å¼**: ç»Ÿä¸€çš„å®¢æˆ·ç«¯åˆ›å»ºæ¥å£
- ğŸ”Œ **å¤šæä¾›å•†æ”¯æŒ**: OpenAI, DeepSeek, Claude, Qwen, Moonshot, Zhipu, Doubao ç­‰
- ğŸ“ **OpenAI ä¼˜å…ˆ**: ä¼˜å…ˆä½¿ç”¨ OpenAI åº“ï¼Œä¿è¯å…¼å®¹æ€§
- âš™ï¸ **é›†ä¸­é…ç½®**: æ‰€æœ‰é…ç½®ç»Ÿä¸€ç®¡ç†ï¼Œæ”¯æŒç¯å¢ƒå˜é‡å’Œå­—å…¸é…ç½®
- ğŸ”„ **æµå¼æ”¯æŒ**: æ”¯æŒåŒæ­¥å’Œæµå¼ä¸¤ç§è°ƒç”¨æ–¹å¼
- ğŸ¯ **ç±»å‹å®‰å…¨**: å®Œæ•´çš„ç±»å‹æ³¨è§£

## æ”¯æŒçš„æä¾›å•†

| æä¾›å•† | ä½¿ç”¨åº“ | é»˜è®¤æ¨¡å‹ |
|--------|--------|----------|
| OpenAI | openai | gpt-4o-mini |
| DeepSeek | openai | deepseek-chat |
| Anthropic (Claude) | anthropic | claude-3-5-sonnet-20241022 |
| Qwen (é€šä¹‰åƒé—®) | openai | qwen-plus |
| Moonshot (æœˆä¹‹æš—é¢) | openai | moonshot-v1-8k |
| Zhipu (æ™ºè°±) | openai | glm-4 |
| Doubao (è±†åŒ…) | openai | doubao-pro-32k |

## å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–ï¼ˆOpenAI åŠå…¼å®¹æä¾›å•†ï¼‰
pip install openai

# Anthropic Claude æ”¯æŒ
pip install anthropic
```

## å¿«é€Ÿå¼€å§‹

### 1. é…ç½®ç¯å¢ƒå˜é‡

åœ¨ `.env` æ–‡ä»¶ä¸­æ·»åŠ ï¼š

```bash
# OpenAI
OPENAI_API_KEY=sk-xxx

# DeepSeek
DEEPSEEK_API_KEY=sk-xxx

# Anthropic Claude
ANTHROPIC_API_KEY=sk-ant-xxx

# Qwen (é€šä¹‰åƒé—®)
QWEN_API_KEY=sk-xxx

# å¯é€‰ï¼šè‡ªå®šä¹‰ base_url
# OPENAI_BASE_URL=https://api.openai.com/v1
```

### 2. åŸºæœ¬ä½¿ç”¨

```python
from core.llm import LLMFactory

# åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä»ç¯å¢ƒå˜é‡è‡ªåŠ¨è¯»å–é…ç½®ï¼‰
client = LLMFactory.create('openai')

# ç®€å•å¯¹è¯
response = client.simple_chat("ä»‹ç»ä¸€ä¸‹ Python")
print(response)

# å®Œæ•´å¯¹è¯
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªhelpfulçš„åŠ©æ‰‹"},
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
]
result = client.chat_completion(messages)
print(result['content'])

# æµå¼å¯¹è¯
for chunk in client.chat_completion_stream(messages):
    print(chunk['content'], end='', flush=True)
```

### 3. ä½¿ç”¨ä¸åŒçš„æä¾›å•†

```python
# DeepSeek
client = LLMFactory.create('deepseek')

# Claude
client = LLMFactory.create('anthropic', model='claude-3-5-sonnet-20241022')

# é€šä¹‰åƒé—®
client = LLMFactory.create('qwen', model='qwen-max')

# æœˆä¹‹æš—é¢
client = LLMFactory.create('moonshot')

# æ™ºè°± AI
client = LLMFactory.create('zhipu')

# è±†åŒ…
client = LLMFactory.create('doubao')
```

### 4. è‡ªå®šä¹‰é…ç½®

```python
from core.llm import LLMFactory, LLMConfig

# æ–¹å¼ 1: ä»å­—å…¸åˆ›å»º
config_dict = {
    'provider': 'openai',
    'api_key': 'sk-xxx',
    'model': 'gpt-4',
    'temperature': 0.8,
    'max_tokens': 2000,
}
client = LLMFactory.create_from_dict(config_dict)

# æ–¹å¼ 2: ä½¿ç”¨é…ç½®å¯¹è±¡
config = LLMConfig.from_dict(config_dict)
client = LLMFactory.create('openai', config=config)
```

### 5. è°ƒç”¨å‚æ•°

```python
# è¦†ç›–é»˜è®¤å‚æ•°
result = client.chat_completion(
    messages=messages,
    temperature=0.9,
    max_tokens=1000,
    top_p=0.95,
)

# æµå¼è°ƒç”¨
for chunk in client.chat_completion_stream(
    messages=messages,
    temperature=0.7,
):
    print(chunk['content'], end='')
```

## å“åº”æ ¼å¼

### åŒæ­¥è°ƒç”¨å“åº”

```python
{
    'content': 'AI å›å¤çš„å†…å®¹',
    'role': 'assistant',
    'model': 'gpt-4o-mini',
    'usage': {
        'prompt_tokens': 10,
        'completion_tokens': 20,
        'total_tokens': 30,
    },
    'finish_reason': 'stop'
}
```

### æµå¼è°ƒç”¨å“åº”

```python
# æ¯ä¸ª chunk
{
    'content': 'å†…å®¹ç‰‡æ®µ',
    'role': 'assistant',
    'finish_reason': None  # æœ€åä¸€ä¸ª chunk ä¸º 'stop'
}
```

## é«˜çº§ç”¨æ³•

### æ³¨å†Œè‡ªå®šä¹‰å®¢æˆ·ç«¯

```python
from core.llm import LLMFactory, BaseLLMClient

class MyCustomClient(BaseLLMClient):
    def _initialize_client(self):
        # å®ç°åˆå§‹åŒ–é€»è¾‘
        pass
    
    def chat_completion(self, messages, **kwargs):
        # å®ç°åŒæ­¥è°ƒç”¨
        pass
    
    def chat_completion_stream(self, messages, **kwargs):
        # å®ç°æµå¼è°ƒç”¨
        pass

# æ³¨å†Œ
LLMFactory.register_client('mycustom', MyCustomClient)

# ä½¿ç”¨
client = LLMFactory.create('mycustom')
```

### æ£€æŸ¥æ”¯æŒçš„æä¾›å•†

```python
# è·å–æ‰€æœ‰æ”¯æŒçš„æä¾›å•†
providers = LLMFactory.get_supported_providers()
print(providers)  # ['openai', 'deepseek', 'anthropic', ...]

# æ£€æŸ¥æ˜¯å¦æ”¯æŒ
is_supported = LLMFactory.is_supported('openai')  # True
```

## é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡

æ¯ä¸ªæä¾›å•†æ”¯æŒä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

- `{PROVIDER}_API_KEY`: API å¯†é’¥ï¼ˆå¿…éœ€ï¼‰
- `{PROVIDER}_BASE_URL`: API åœ°å€ï¼ˆå¯é€‰ï¼‰
- `{PROVIDER}_MODEL`: é»˜è®¤æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
- `{PROVIDER}_MAX_TOKENS`: æœ€å¤§ token æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ 4096ï¼‰
- `{PROVIDER}_TEMPERATURE`: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ 0.7ï¼‰
- `{PROVIDER}_TIMEOUT`: è¶…æ—¶æ—¶é—´ï¼ˆå¯é€‰ï¼Œé»˜è®¤ 60 ç§’ï¼‰

ä¾‹å¦‚ï¼š
```bash
OPENAI_API_KEY=sk-xxx
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4
OPENAI_TEMPERATURE=0.8
```

### é…ç½®ä¼˜å…ˆçº§

è°ƒç”¨å‚æ•° > é…ç½®å¯¹è±¡ > ç¯å¢ƒå˜é‡ > é»˜è®¤é…ç½®

## é”™è¯¯å¤„ç†

```python
try:
    client = LLMFactory.create('openai')
    result = client.chat_completion(messages)
except ValueError as e:
    print(f"é…ç½®é”™è¯¯: {e}")
except RuntimeError as e:
    print(f"API è°ƒç”¨å¤±è´¥: {e}")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```

## å®é™…åº”ç”¨ç¤ºä¾‹

### Django è§†å›¾ä¸­ä½¿ç”¨

```python
from django.http import JsonResponse
from core.llm import LLMFactory

def chat_api(request):
    user_message = request.POST.get('message')
    
    client = LLMFactory.create('deepseek')
    response = client.simple_chat(user_message)
    
    return JsonResponse({'reply': response})
```

### æµå¼å“åº”

```python
from django.http import StreamingHttpResponse
from core.llm import LLMFactory

def chat_stream_api(request):
    messages = [{"role": "user", "content": request.POST.get('message')}]
    
    client = LLMFactory.create('openai')
    
    def generate():
        for chunk in client.chat_completion_stream(messages):
            yield chunk['content']
    
    return StreamingHttpResponse(generate(), content_type='text/plain')
```

### æ‰¹é‡å¤„ç†

```python
from core.llm import LLMFactory

def batch_summarize(texts: list[str]):
    client = LLMFactory.create('deepseek')
    
    results = []
    for text in texts:
        summary = client.simple_chat(
            f"è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\n\n{text}",
            temperature=0.3
        )
        results.append(summary)
    
    return results
```

## æ€§èƒ½å»ºè®®

1. **å¤ç”¨å®¢æˆ·ç«¯**: å®¢æˆ·ç«¯å¯¹è±¡å¯ä»¥å¤ç”¨ï¼Œé¿å…é¢‘ç¹åˆ›å»º
2. **åˆç†è®¾ç½®è¶…æ—¶**: æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ timeout å‚æ•°
3. **ä½¿ç”¨æµå¼**: é•¿æ–‡æœ¬ç”Ÿæˆå»ºè®®ä½¿ç”¨æµå¼æ¥å£
4. **é€‰æ‹©åˆé€‚çš„æ¨¡å‹**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æ¨¡å‹

## æ³¨æ„äº‹é¡¹

1. ç¡®ä¿ç¯å¢ƒå˜é‡å·²æ­£ç¡®é…ç½®
2. Anthropic éœ€è¦å•ç‹¬å®‰è£… `anthropic` åº“
3. ä¸åŒæä¾›å•†çš„ API é™æµç­–ç•¥ä¸åŒï¼Œæ³¨æ„æ§åˆ¶è¯·æ±‚é¢‘ç‡
4. æ•æ„Ÿçš„ API Key ä¸è¦ç¡¬ç¼–ç åœ¨ä»£ç ä¸­

## æ‰©å±•å¼€å‘

å¦‚éœ€æ·»åŠ æ–°çš„æä¾›å•†ï¼š

1. ç»§æ‰¿ `BaseLLMClient` ç±»
2. å®ç°æŠ½è±¡æ–¹æ³•
3. åœ¨ `LLMFactory` ä¸­æ³¨å†Œ
4. æ›´æ–°é…ç½®æ–‡ä»¶

è¯¦è§ä»£ç æ³¨é‡Šå’Œç¤ºä¾‹ã€‚
