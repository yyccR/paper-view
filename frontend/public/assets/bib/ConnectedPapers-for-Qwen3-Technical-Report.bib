@article{d2d84d56f730f81d276a02b48d5d44db5bde0b4a,
title = {Qwen3 Technical Report},
year = {2025},
url = {https://www.semanticscholar.org/paper/d2d84d56f730f81d276a02b48d5d44db5bde0b4a},
abstract = {In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.},
author = {An Yang and Anfeng Li and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Gao and Chengen Huang and Chenxu Lv and Chujie Zheng and Dayiheng Liu and Fan Zhou and Fei Huang and Feng Hu and Hao Ge and Haoran Wei and Huan Lin and Jialong Tang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxin Yang and Jingren Zhou and Jingren Zhou and Junyan Lin and Kai Dang and Keqin Bao and Ke‚ÄêPei Yang and Le Yu and Li-Chun Deng and Mei Li and Min Xue and Mingze Li and Pei Zhang and Peng Wang and Qin Zhu and Rui Men and Ruize Gao and Shi-Qiang Liu and Shuang Luo and Tianhao Li and Tianyi Tang and Wenbiao Yin and Xingzhang Ren and Xinyu Wang and Xinyu Zhang and Xuancheng Ren and Yang Fan and Yang Su and Yi-Chao Zhang and Yinger Zhang and Yu Wan and Yuqiong Liu and Zekun Wang and Zeyu Cui and Zhenru Zhang and Zhipeng Zhou and Zihan Qiu},
journal = {ArXiv},
volume = {abs/2505.09388},
pages = {null},
doi = {10.48550/arXiv.2505.09388},
arxivid = {2505.09388},
}

@article{143e18bfd7c356592e7c1439738a3525d3e16279,
title = {Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?},
year = {2025},
url = {https://www.semanticscholar.org/paper/143e18bfd7c356592e7c1439738a3525d3e16279},
abstract = {Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly on mathematics and programming tasks. Similar to how traditional RL helps agents explore and learn new strategies, RLVR is believed to enable LLMs to continuously self-improve, thus acquiring novel reasoning abilities beyond those of the corresponding base models. In this study we critically examine the current state of RLVR by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across various model families, RL algorithms, and math, coding, and visual reasoning benchmarks, using pass@k at large k values as the evaluation metric. Surprisingly, we find that the current training setup does not elicit fundamentally new reasoning patterns. While RLVR-trained models outperform their base models at small k (e.g., k = 1), the base models achieve a higher pass@k score when k is large. Coverage and perplexity analyses show that the observed reasoning abilities originate from and are bounded by the base model. Treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in leveraging the potential of the base model. By contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model's reasoning capabilities. Overall, our findings suggest that current RLVR methods have not yet realized the potential of RL to elicit truly novel reasoning abilities in LLMs. This highlights the need for improved RL paradigms, such as continual scaling and multi-turn agent-environment interaction, to unlock this potential.},
author = {Yang Yue and Zhiqi Chen and Rui Lu and Andrew Zhao and Zhaokai Wang and Shiji Song and Gao Huang},
journal = {ArXiv},
volume = {abs/2504.13837},
pages = {null},
doi = {10.48550/arXiv.2504.13837},
arxivid = {2504.13837},
}

@article{6bf5b8c42d913ba65d0398871d66a223e79f428d,
title = {Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts},
year = {2025},
url = {https://www.semanticscholar.org/paper/6bf5b8c42d913ba65d0398871d66a223e79f428d},
abstract = {The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate scalability by enabling sparse parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size.},
author = {Haoyuan Wu and Haoxing Chen and Xiaodong Chen and Zhanchao Zhou and Tieyuan Chen and Yihong Zhuang and Guoshan Lu and Zenan Huang and Junbo Zhao and Lin Liu and Zhenzhong Lan and Bei Yu and Jianguo Li},
journal = {ArXiv},
volume = {abs/2508.07785},
pages = {null},
doi = {10.48550/arXiv.2508.07785},
arxivid = {2508.07785},
}

@article{97e0a3548a6f0818262b4f86e4180f256e2f0128,
title = {MiMo: Unlocking the Reasoning Potential of Language Model - From Pretraining to Posttraining},
year = {2025},
url = {https://www.semanticscholar.org/paper/97e0a3548a6f0818262b4f86e4180f256e2f0128},
abstract = {We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.},
author = {Xi Xia and Bowen Shen and Cici and Dawei Zhu and Di Zhang and Gang Wang and Hailin Zhang and Huaqiu Liu and Jiebao Xiao and Jinhao Dong and Liang Zhao and Peidian Li and Peng Wang and Shi-jie Yu and Shimao Chen and Weikun Wang and Wenhan Ma and Xia Deng and Yi Huang and Yi-Hao Song and Zi-Ang Jiang and Bowen Ye and Can Cai and Chenhong He and Dong Zhang and Duo Zhang and Guoan Wang and Hao Tian and Hao-Yun Zhao and Hengxu Qu and Hong-Mei Xu and Jun-Miao Shi and Kainan Bao and Qing Fang and Kang Zhou and Kang Zhou and Lei Li and Menghang Zhu and Nuo Chen and Qiantong Wang and Shao-yang Liu and Shicheng Li and Shuhao Gu and Shu-Qin Ren and Shuo Liu and Sirui Deng and Weiji Zhuang and Weiwei Lv and Wenyu Yang and Xin Zhang and Xing Yong and Xing Zhang and Xi-Na Song and Xin-Ming Xu and Xu Wang and Yihan Yan and Yunwu Tu and Yu-Shi Tian and Yudong Wang and Yue Yu and Zhenrui Lin and Zhichao Song and Zihao Yue},
journal = {ArXiv},
volume = {abs/2505.07608},
pages = {null},
doi = {10.48550/arXiv.2505.07608},
arxivid = {2505.07608},
}

@article{7d6d6211e439cf976ed83950ae12e53649c3cd68,
title = {Param$\Delta$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost},
year = {2025},
url = {https://www.semanticscholar.org/paper/7d6d6211e439cf976ed83950ae12e53649c3cd68},
abstract = {The post-training phase of large language models is essential for enhancing capabilities such as instruction-following, reasoning, and alignment with human preferences. However, it demands extensive high-quality data and poses risks like overfitting, alongside significant computational costs due to repeated post-training and evaluation after each base model update. This paper introduces $Param\Delta$, a novel method that streamlines post-training by transferring knowledge from an existing post-trained model to a newly updated base model with ZERO additional training. By computing the difference between post-trained model weights ($\Theta_\text{post}$) and base model weights ($\Theta_\text{base}$), and adding this to the updated base model ($\Theta'_\text{base}$), we define $Param\Delta$ Model as: $\Theta_{\text{Param}\Delta} = \Theta_\text{post} - \Theta_\text{base} + \Theta'_\text{base}$. This approach surprisingly equips the new base model with post-trained capabilities, achieving performance comparable to direct post-training. We did analysis on LLama3, Llama3.1, Qwen, and DeepSeek-distilled models. Results indicate $Param\Delta$ Model effectively replicates traditional post-training. For example, the $Param\Delta$ Model obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains approximately 95\% of Llama3.1-inst model's performance on average. $Param\Delta$ brings a new perspective on how to fully leverage models in the open-weight community, where checkpoints for base and instruct models are readily available and frequently updated, by providing a cost-free framework to accelerate the iterative cycle of model development.},
author = {Sheng Cao and Mingrui Wu and Karthik Prasad and Yuandong Tian and Zechun Liu},
arxivid = {2504.21023},
}

@article{ef5bc291f7753d494bbef4aef4c1e2afeaeb1b30,
title = {Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset},
year = {2025},
url = {https://www.semanticscholar.org/paper/ef5bc291f7753d494bbef4aef4c1e2afeaeb1b30},
abstract = {Pretraining large language models (LLMs) on high-quality, structured data such as mathematics and code substantially enhances reasoning capabilities. However, existing math-focused datasets built from Common Crawl suffer from degraded quality due to brittle extraction heuristics, lossy HTML-to-text conversion, and the failure to reliably preserve mathematical structure. In this work, we introduce Nemotron-CC-Math, a large-scale, high-quality mathematical corpus constructed from Common Crawl using a novel, domain-agnostic pipeline specifically designed for robust scientific text extraction. Unlike previous efforts, our pipeline recovers math across various formats (e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx and a targeted LLM-based cleaning stage. This approach preserves the structural integrity of equations and code blocks while removing boilerplate, standardizing notation into LaTeX representation, and correcting inconsistencies. We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+ (133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably, Nemotron-CC-Math-4+ not only surpasses all prior open math datasets-including MegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens than FineMath-4+, which was previously the highest-quality math pretraining dataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to +12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines, while also improving general-domain performance on MMLU and MMLU-Stem. We present the first pipeline to reliably extract scientific content--including math--from noisy web-scale data, yielding measurable gains in math, code, and general reasoning, and setting a new state of the art among open math pretraining corpora. To support open-source efforts, we release our code and datasets.},
author = {Rabeeh Karimi Mahabadi and S. Satheesh and Shrimai Prabhumoye and M. Patwary and M. Shoeybi and Bryan Catanzaro},
journal = {ArXiv},
volume = {abs/2508.15096},
pages = {null},
doi = {10.48550/arXiv.2508.15096},
arxivid = {2508.15096},
}

@article{54a84b3767f2a43802f4036346c7db6304a9703c,
title = {Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling},
year = {2025},
url = {https://www.semanticscholar.org/paper/54a84b3767f2a43802f4036346c7db6304a9703c},
abstract = {A large language model (LLM) with knowledge in both scientific and general tasks is the foundation of science general intelligence. However, directly continued pretraining an LLM using science data usually leads to catastrophic forgetting, which indicates severe degradation in general ability. In this report, we present Innovator, which solves this problem by upcycling a pre-trained dense LLM into a fine-grained Mixtures-of-Experts model during continued pretraining, where different experts are expected to learn science knowledge in different disciplines, and a shared expert is utilized for general tasks. Innovator introduces a four-stage upcycle training paradigm: (1) Scientific Expert Induction on discipline-specific data, (2) Fine-grained Expert Splitting via FFN dimension decomposition, (3) Science-Aware Routing warmup, and (4) Generalist-Scientist Integration training on hybrid datasets. Such a paradigm enables knowledge in the general domain, and different scientific disciplines can be decoupled, avoiding the negative influence among knowledge in different domains. With 53.3B total parameters and 13.3B activated, Innovator extends Qwen2.5-7B using a shared general expert and 64 specialized scientific experts with 8 activated. Trained on 300B tokens with tri-level quality-controlled data, Innovator achieves 25% average improvement across 30 scientific tasks with a win rate as 70%, while retaining 99% performance in general tasks. Furthermore, Innovator-Reason, which is post-trained from Innovator for reasoning boosting, exhibits excellent reasoning performance in solving complex scientific problems with improvements over 30%.},
author = {Ning Liao and Xiaoxing Wang and Zehao Lin and Weiyang Guo and Feng Hong and Shixiang Song and Geng Yu and Zihua Zhao and Sitao Xie and Longxuan Wei and Xiangqi Jin and Xiaohan Qin and Jiale Ma and Kai Chen and Jiangchao Yao and Zhouhan Lin and Junchi Yan and Zhiyu Li and Feiyu Xiong and Yanfeng Wang and Linfeng Zhang},
journal = {ArXiv},
volume = {abs/2507.18671},
pages = {null},
doi = {10.48550/arXiv.2507.18671},
arxivid = {2507.18671},
}

@article{11dd2fc88747d20629f5aafab72ba649d93e969c,
title = {Qwen2.5-Coder Technical Report},
year = {2024},
url = {https://www.semanticscholar.org/paper/11dd2fc88747d20629f5aafab72ba649d93e969c},
abstract = {In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.},
author = {Binyuan Hui and Jian Yang and Zeyu Cui and Jiaxi Yang and Dayiheng Liu and Lei Zhang and Tianyu Liu and Jiajun Zhang and Bowen Yu and Kai Dang and An Yang and Rui Men and Fei Huang and Shanghaoran Quan and Xingzhang Ren and Xuancheng Ren and Jingren Zhou and Junyang Lin},
journal = {ArXiv},
volume = {abs/2409.12186},
pages = {null},
doi = {10.48550/arXiv.2409.12186},
arxivid = {2409.12186},
}

@article{a7ad5a50dc5d651d9182471ec32470d4f1b645e8,
title = {Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent},
year = {2024},
url = {https://www.semanticscholar.org/paper/a7ad5a50dc5d651d9182471ec32470d4f1b645e8},
abstract = {In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications. Codes: https://github.com/Tencent/Hunyuan-Large Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large},
author = {Xingwu Sun and Yanfeng Chen and Yiqing Huang and Ruobing Xie and Jiaqi Zhu and Kai Zhang and Shuaipeng Li and Zhen Yang and Jonny Han and Xiaobo Shu and Jiahao Bu and Zhongzhi Chen and Xuemeng Huang and Feng Lian and Saiyong Yang and Jianfeng Yan and Yuyuan Zeng and Xiaoqing Ren and Chao Yu and Lulu Wu and Yue Mao and Jun Xia and Tao Yang and Suncong Zheng and Kan Wu and Dian Jiao and Jinbao Xue and Xipeng Zhang and Decheng Wu and Kai Liu and Dengpeng Wu and Guanghui Xu and Shaohua Chen and Shuang Chen and Xiaowei Feng and Yigeng Hong and Junqiang Zheng and Chengcheng Xu and Zong-Rui Li and Xi Kuang and Jian-hua Hu and Yiqi Chen and Yuchi Deng and Guiyang Li and Ao Liu and Chenchen Zhang and Shi-He Hu and Zilong Zhao and Zi-Hao Wu and Yao Ding and Weichao Wang and Han Liu and Roberts Wang and Haoyang Fei and Peijie Yu and Ze Zhao and Xun Cao and Hai Wang and Fusheng Xiang and Meng-Sheng Huang and Zhiyu Xiong and Bin Hu and Xue-yan Hou and Lei Jiang and Jia-bing Ma and Jiajia Wu and Yaping Deng and Yi Shen and Qian Wang and Weijie Liu and Jie Liu and Meng Chen and Liang Dong and Wei Jia and Hu Chen and Feifei Liu and Ruixin Yuan and Huilin Xu and Zhenxiang Yan and Tengfei Cao and Zhichao Hu and Xinhua Feng and Dong Du and Ting-Ting Yu and Yang-Dan Tao and Feng Zhang and Jianchen Zhu and Chengzhong Xu and Xirui Li and Chong Zha and Ouyang Wen and Yi Xia and Xiang Li and Ze He and Rongpeng Chen and Jiawei Song and Ruibin Chen and Fan Jiang and Chongqing Zhao and Bo Wang},
journal = {ArXiv},
volume = {abs/2411.02265},
pages = {null},
doi = {10.48550/arXiv.2411.02265},
arxivid = {2411.02265},
}

@article{34471a2fa18ea22efad5287cf4aeb18542c98a9b,
title = {DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
year = {2025},
url = {https://www.semanticscholar.org/paper/34471a2fa18ea22efad5287cf4aeb18542c98a9b},
abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
author = {DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Jun-Mei Song and Ruoyu Zhang and R. Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiaoling Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and A. Liu and Bing Xue and Bing-Li Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and C. Deng and Chenyu Zhang and C. Ruan and Damai Dai and Deli Chen and Dong-Li Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Yifan Shi and Yi Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yu-Jing Zou and Yujia He and Yunfan Xiong and Yu-Wei Luo and Yu-mei You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanping Huang and Yao Li and Yi Zheng and Yuchen Zhu and Yunxiang Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Ren and Z. Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhen-guo Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zi-An Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
journal = {ArXiv},
volume = {abs/2501.12948},
pages = {null},
doi = {10.48550/arXiv.2501.12948},
arxivid = {2501.12948},
}

@article{71b7a18e75dc540c889be080f4206460359ea20d,
title = {Qwen2.5-1M Technical Report},
year = {2025},
url = {https://www.semanticscholar.org/paper/71b7a18e75dc540c889be080f4206460359ea20d},
abstract = {We introduce Qwen2.5-1M, a series of models that extend the context length to 1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series have significantly enhanced long-context capabilities through long-context pre-training and post-training. Key techniques such as long data synthesis, progressive pre-training, and multi-stage supervised fine-tuning are employed to effectively enhance long-context performance while reducing training costs. To promote the use of long-context models among a broader user base, we present and open-source our inference framework. This framework includes a length extrapolation method that can expand the model context lengths by at least four times, or even more, without additional training. To reduce inference costs, we implement a sparse attention method along with chunked prefill optimization for deployment scenarios and a sparsity refinement method to improve precision. Additionally, we detail our optimizations in the inference engine, including kernel optimization, pipeline parallelism, and scheduling optimization, which significantly enhance overall inference performance. By leveraging our inference framework, the Qwen2.5-1M models achieve a remarkable 3x to 7x prefill speedup in scenarios with 1 million tokens of context. This framework provides an efficient and powerful solution for developing applications that require long-context processing using open-source models. The Qwen2.5-1M series currently includes the open-source models Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, as well as the API-accessed model Qwen2.5-Turbo. Evaluations show that Qwen2.5-1M models have been greatly improved in long-context tasks without compromising performance in short-context scenarios. Specifically, the Qwen2.5-14B-Instruct-1M model significantly outperforms GPT-4o-mini in long-context tasks and supports contexts eight times longer.},
author = {An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zi-Yan Zhang},
journal = {ArXiv},
volume = {abs/2501.15383},
pages = {null},
doi = {10.48550/arXiv.2501.15383},
arxivid = {2501.15383},
}

@article{dd4cfde3e135f799a9a71b4f57e13a29de89f7e3,
title = {DAPO: An Open-Source LLM Reinforcement Learning System at Scale},
year = {2025},
url = {https://www.semanticscholar.org/paper/dd4cfde3e135f799a9a71b4f57e13a29de89f7e3},
abstract = {Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the $\textbf{D}$ecoupled Clip and $\textbf{D}$ynamic s$\textbf{A}$mpling $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{DAPO}$) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.},
author = {Qiying Yu and Zheng Zhang and Ruofei Zhu and Yufeng Yuan and Xiaochen Zuo and Yu Yue and Tiantian Fan and Gaohong Liu and Lingjun Liu and Xin Liu and Haibin Lin and Zhiqi Lin and Bole Ma and Guangming Sheng and Yuxuan Tong and Chi Zhang and Mofan Zhang and Wang Zhang and Hang Zhu and Jinhua Zhu and Jiaze Chen and Jiangjie Chen and Chengyi Wang and Honglin Yu and Weinan Dai and Yuxuan Song and Xiang Wei and Haodong Zhou and Jingjing Liu and Wei Ma and Ya-Qin Zhang and Lin Yan and Mu Qiao and Yong-Xu Wu and Mingxuan Wang},
journal = {ArXiv},
volume = {abs/2503.14476},
pages = {null},
doi = {10.48550/arXiv.2503.14476},
arxivid = {2503.14476},
}

@article{7260442ef9c0448f07ce3803efd49cebaffcebe9,
title = {DeepSeek LLM: Scaling Open-Source Language Models with Longtermism},
year = {2024},
url = {https://www.semanticscholar.org/paper/7260442ef9c0448f07ce3803efd49cebaffcebe9},
abstract = {The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.},
author = {DeepSeek-AI Xiao Bi and Deli Chen and Guanting Chen and Shanhuang Chen and Damai Dai and C. Deng and Honghui Ding and Kai Dong and Qiushi Du and Zhe Fu and Huazuo Gao and Kaige Gao and Wenjun Gao and Ruiqi Ge and Kang Guan and Daya Guo and Jianzhong Guo and Guangbo Hao and Zhewen Hao and Ying He and Wen-Hui Hu and Panpan Huang and Erhang Li and Guowei Li and Jiashi Li and Yao Li and Y. K. Li and W. Liang and Fangyun Lin and A. Liu and Bo Liu (Benjamin Liu) and Wen Liu and Xiaodong Liu and Xin Liu and Yiyuan Liu and Haoyu Lu and Shanghao Lu and Fuli Luo and Shirong Ma and X. Nie and Tian Pei and Yishi Piao and Junjie Qiu and Hui Qu and Tongzheng Ren and Z. Ren and C. Ruan and Zhangli Sha and Zhihong Shao and Jun-Mei Song and Xuecheng Su and Jingxiang Sun and Yaofeng Sun and Min Tang and Bing-Li Wang and Peiyi Wang and Shiyu Wang and Yaohui Wang and Yongji Wang and Tong Wu and Yu Wu and Xin Xie and Zhenda Xie and Ziwei Xie and Yi Xiong and Hanwei Xu and R. X. Xu and Yanhong Xu and Dejian Yang and Yu-mei You and Shuiping Yu and Xin-yuan Yu and Bo Zhang and Haowei Zhang and Lecong Zhang and Liyue Zhang and Mingchuan Zhang and Minghu Zhang and Wentao Zhang and Yichao Zhang and Chenggang Zhao and Yao Zhao and Shangyan Zhou and Shunfeng Zhou and Qihao Zhu and Yuheng Zou},
journal = {ArXiv},
volume = {abs/2401.02954},
pages = {null},
doi = {10.48550/arXiv.2401.02954},
arxivid = {2401.02954},
}

@article{54fb839f621e3fe787437ab8ca5f37e7e4726bfe,
title = {Qwen2 Technical Report},
year = {2024},
url = {https://www.semanticscholar.org/paper/54fb839f621e3fe787437ab8ca5f37e7e4726bfe},
abstract = {This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.},
author = {An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Ke-Yang Chen and Kexin Yang and Mei Li and Min Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yunyang Wan and Yunfei Chu and Zeyu Cui and Zhenru Zhang and Zhi-Wei Fan},
journal = {ArXiv},
volume = {abs/2407.10671},
pages = {null},
doi = {10.48550/arXiv.2407.10671},
arxivid = {2407.10671},
}

@article{77904afcad72c97335e9d95a585a2f73c744ce01,
title = {CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction},
year = {2025},
url = {https://www.semanticscholar.org/paper/77904afcad72c97335e9d95a585a2f73c744ce01},
abstract = {Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math&numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at https://github.com/hkust-nlp/CodeIO.},
author = {Junlong Li and Daya Guo and Dejian Yang and Runxin Xu and Yu Wu and Junxian He},
journal = {ArXiv},
volume = {abs/2502.07316},
pages = {null},
doi = {10.48550/arXiv.2502.07316},
arxivid = {2502.07316},
}

@article{7d8a61500e4d373fc77f81d0640f70f994bd9d79,
title = {BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models},
year = {2025},
url = {https://www.semanticscholar.org/paper/7d8a61500e4d373fc77f81d0640f70f994bd9d79},
abstract = {Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.},
author = {Xu Huang and Wenhao Zhu and Hanxu Hu and Conghui He and Lei Li and Shujian Huang and Fei Yuan},
journal = {ArXiv},
volume = {abs/2502.07346},
pages = {null},
doi = {10.48550/arXiv.2502.07346},
arxivid = {2502.07346},
}

@article{35b142ea69598e6241f0011312128031df55895c,
title = {DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models},
year = {2024},
url = {https://www.semanticscholar.org/paper/35b142ea69598e6241f0011312128031df55895c},
abstract = {Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.},
author = {Zhihong Shao and Peiyi Wang and Qihao Zhu and R. Xu and Jun-Mei Song and Mingchuan Zhang and Y. K. Li and Yu Wu and Daya Guo},
journal = {ArXiv},
volume = {abs/2402.03300},
pages = {null},
doi = {10.48550/arXiv.2402.03300},
arxivid = {2402.03300},
}

@article{8343f9745b8450f179a150a0af4eaae94dbbb6e1,
title = {Kimi K2: Open Agentic Intelligence},
year = {2025},
url = {https://www.semanticscholar.org/paper/8343f9745b8450f179a150a0af4eaae94dbbb6e1},
abstract = {We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments. Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.},
author = {Kimi Team Yifan Bai and Yiping Bao and Guanduo Chen and Jiahao Chen and Ningxin Chen and Ruijue Chen and Yanru Chen and Yuankun Chen and Yutian Chen and Zhuofu Chen and Jialei Cui and Haochen Ding and Meng-Xiao Dong and Angang Du and Chenzhuang Du and Dikang Du and Yulun Du and Yu Fan and Yichen Feng and Kelin Fu and Bofei Gao and Hongcheng Gao and Peizhong Gao and Tong Gao and Xinran Gu and Longyu Guan and Haiqing Guo and Jia-Xing Guo and Hao-Xing Hu and Xiaoru Hao and Tianhong He and Weiran He and Wen He and Chao Hong and Yan-Ni Hu and Zhenxing Hu and Weixiao Huang and Zhiqi Huang and Zihao Huang and Tao Jiang and Zhejun Jiang and Xinyi Jin and Yongsheng Kang and Guokun Lai and Cheng Li and Fang Li and Haoyang Li and Ming Li and Wentao Li and Yanhao Li and Xiao-Ming Xie and Weiming Xiong and Boyu Xu and Jing Xu and Jinjing Xu and L. H. Xu and Lin Xu and Suting Xu and Weixin Xu and Xinran Xu and Yangchuan Xu and Zi-Yang Xu and Junjie Yan and Yuzi Yan and Xiaofei Yang and Ying Yang and Zhengqi Yang and Zhilin Yang and Zonghan Yang and Haotian Yao and Xingcheng Yao and Wen-guang Ye and Zhuorui Ye and Bohong Yin and Long Yu and Enming Yuan and Hongbang Yuan and Mengjie Yuan and Haobing Zhan and Dehao Zhang and Hao Zhang and Wanlu Zhang and Xiaobin Zhang and Yangkun Zhang and Yizhi Zhang and Yongting Zhang and Yu Zhang and Yutao Zhang and Yutong Zhang and Zheng Zhang and Hao-Dong Zhao and Yikai Zhao and Huabin Zheng and Shao-Jiang Zheng and Jianren Zhou and Xinyu Zhou and Zaida Zhou and Zhengxin Zhu and Weiyu Zhuang and Xinxing Zu},
journal = {ArXiv},
volume = {abs/2507.20534},
pages = {null},
doi = {10.48550/arXiv.2507.20534},
arxivid = {2507.20534},
}

@article{7c437ef23031325e2449f2d7813c55dd5f559ecd,
title = {Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs},
year = {2025},
url = {https://www.semanticscholar.org/paper/7c437ef23031325e2449f2d7813c55dd5f559ecd},
abstract = {Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference.},
author = {Yehui Tang and Yichun Yin and Yaoyuan Wang and Hang Zhou and Yu Pan and Wei Guo and Ziyang Zhang and Miao Rang and Fangcheng Liu and Naifu Zhang and Binghan Li and Yong Dong and Xiaojun Meng and Yasheng Wang and Dong Li and Yin Li and Dandan Tu and Can Chen and Youliang Yan and Fisher Yu and Ruiming Tang and Yunhe Wang and Botian Huang and Bo Wang and Boxiao Liu and Changzheng Zhang and Da Kuang and Fei Liu and Gang Huang and Jiansheng Wei and Jiarui Qin and Jie Ran and Jinpeng Li and Jun Zhao and Liang Dai and Lin Li and Li-e Deng and Peifeng Qin and Peng Zeng and Qiang Gu and Shao-Ying Tang and Shengjun Cheng and Tao Gao and Tao Yu and Tianshu Li and Tianyu Bi and Wei He and Weikai Mao and Wen-Tsung Huang and Wulong Liu and Xiabing Li and Xian Yu and Xue-ying Wu and Xudong He and Yangkai Du and Yan Xu and Yecen Tian and Yimeng Wu and Yong-Hui Huang and Yong Tian and Yong Zhu and Yue Li and Yufei Wang and Yuhang Gai and Yujun Li and Yuankai Luo and Yunsheng Ni and Yusen Sun and Zelin Chen and Zhe Liu and Zhicheng Liu and Zhipeng Tu and Zilin Ding and Zongyuan Zhan},
journal = {ArXiv},
volume = {abs/2505.04519},
pages = {null},
doi = {10.48550/arXiv.2505.04519},
arxivid = {2505.04519},
}

@article{93ca54fa0fbf2dae113e5433154ffb49eadecd04,
title = {Yi-Lightning Technical Report},
year = {2024},
url = {https://www.semanticscholar.org/paper/93ca54fa0fbf2dae113e5433154ffb49eadecd04},
abstract = {This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at https://platform.lingyiwanwu.com.},
author = {01.AI Alan Wake and Albert Wang and Bei Chen and C. Lv and Chao Li and Chengen Huang and Chenglin Cai and Chujie Zheng and Daniel Cooper and Ethan Dai and Fan Zhou and Feng Hu and Heng Ji and Howard Qiu and Jiangcheng Zhu and Jun Tian and Katherine Su and Lihua Zhang and Liying Li and Ming Song and Mou Li and Peng Liu and Qichen Hu and Shawn Wang and Shijun Zhou and Shi-Yin Li and Tianhang Zhu and Wen Xie and Xiang He and Xiaobo Chen and Xiaohui Hu and Xiaoyi Ren and Xinyao Niu and Yanpeng Li and Yongke Zhao and Yongzhen Luo and Yuchi Xu and Yuxuan Sha and Zhaodong Yan and Zhi-Yan Liu and Zi-Ke Zhang},
journal = {ArXiv},
volume = {abs/2412.01253},
pages = {null},
doi = {10.48550/arXiv.2412.01253},
arxivid = {2412.01253},
}

@article{176c6ed55d03bf27f5fe2550310773d6d424965e,
title = {xGen-small Technical Report},
year = {2025},
url = {https://www.semanticscholar.org/paper/176c6ed55d03bf27f5fe2550310773d6d424965e},
abstract = {We introduce xGen-small, a family of 4B and 9B Transformer decoder models optimized for long-context applications. Our vertically integrated pipeline unites domain-balanced, frequency-aware data curation; multi-stage pre-training with quality annealing and length extension to 128k tokens; and targeted post-training via supervised fine-tuning, preference learning, and online reinforcement learning. xGen-small delivers strong performance across various tasks, especially in math and coding domains, while excelling at long context benchmarks.},
author = {Erik Nijkamp and Bo Pang and Egor Pakhomov and Akash Gokul and Jin Qu and Silvio Savarese and Yingbo Zhou and Caiming Xiong},
journal = {ArXiv},
volume = {abs/2505.06496},
pages = {null},
doi = {10.48550/arXiv.2505.06496},
arxivid = {2505.06496},
}

@article{72664a573fa42ae425aaa2471f202d67ec00b51b,
title = {Hierarchical Balance Packing: Towards Efficient Supervised Fine-tuning for Long-Context LLM},
year = {2025},
url = {https://www.semanticscholar.org/paper/72664a573fa42ae425aaa2471f202d67ec00b51b},
abstract = {Training Long-Context Large Language Models (LLMs) is challenging, as hybrid training with long-context and short-context data often leads to workload imbalances. Existing works mainly use data packing to alleviate this issue but fail to consider imbalanced attention computation and wasted communication overhead. This paper proposes Hierarchical Balance Packing (HBP), which designs a novel batch-construction method and training recipe to address those inefficiencies. In particular, the HBP constructs multi-level data packing groups, each optimized with a distinct packing length. It assigns training samples to their optimal groups and configures each group with the most effective settings, including sequential parallelism degree and gradient checkpointing configuration. To effectively utilize multi-level groups of data, we design a dynamic training pipeline specifically tailored to HBP, including curriculum learning, adaptive sequential parallelism, and stable loss. Our extensive experiments demonstrate that our method significantly reduces training time over multiple datasets and open-source models while maintaining strong performance. For the largest DeepSeek-V2 (236B) MOE model, our method speeds up the training by 2.4$\times$ with competitive performance.},
author = {Yongqiang Yao and Jingru Tan and Kaihuan Liang and Feizhao Zhang and Yazhe Niu and Jiahao Hu and Ruihao Gong and Dahua Lin and Ningyi Xu},
journal = {ArXiv},
volume = {abs/2503.07680},
pages = {null},
doi = {10.48550/arXiv.2503.07680},
arxivid = {2503.07680},
}

@article{b8ec3b451c82cc11ef230a2a29d0ab008ee65d38,
title = {Technical Report of TeleChat2, TeleChat2.5 and T1},
year = {2025},
url = {https://www.semanticscholar.org/paper/b8ec3b451c82cc11ef230a2a29d0ab008ee65d38},
abstract = {We introduce the latest series of TeleChat models: \textbf{TeleChat2}, \textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over their predecessor, TeleChat. Despite minimal changes to the model architecture, the new series achieves substantial performance gains through enhanced training strategies in both pre-training and post-training stages. The series begins with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion high-quality and diverse tokens. This is followed by Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to further enhance its capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by incorporating a continual pretraining phase with domain-specific datasets, combined with reinforcement learning (RL) to improve performance in code generation and mathematical reasoning tasks. The \textbf{T1} variant is designed for complex reasoning, supporting long Chain-of-Thought (CoT) reasoning and demonstrating substantial improvements in mathematics and coding. In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are dense Transformer-based architectures with 115B parameters, showcasing significant advancements in reasoning and general task performance compared to the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2}, \textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B and 115B parameters, to empower developers and researchers with state-of-the-art language models tailored for diverse applications.},
author = {Zihan Wang and Xinzhan Liu and Yitong Yao and Chao Wang and Yu Zhao and Zhihao Yang and Wenming Deng and Kaipeng Jia and Jiaxin Peng and Yuyao Huang and Sishi Xiong and Zhuoru Jiang and Kaidong Yu and Xiaohui Hu and Fubei Yao and Ruiyu Fang and Zhuoru Jiang and Rui Song and Qiyi Xie and Rui Xue and Xuewei He and Yan-Xue Xue and Zhu Yuan and Zhaoxi Zhang and Zilu Huang and Shiquan Wang and Xin Wang and Hanming Wu and Mingyuan Wang and Xufeng Zhan and Yuhan Sun and Zhaohu Xing and Yuhao Jiang and Bingkai Yang and Shuangyong Song and Yongxiang Li and Zhongjiang He and Xuelong Li},
journal = {ArXiv},
volume = {abs/2507.18013},
pages = {null},
doi = {10.48550/arXiv.2507.18013},
arxivid = {2507.18013},
}

@article{fe100060a35c19a6c538e0453cdf3662ec015199,
title = {Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models},
year = {2025},
url = {https://www.semanticscholar.org/paper/fe100060a35c19a6c538e0453cdf3662ec015199},
abstract = {Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large Language Models (LLMs) efficiently by decoupling total parameters from computational cost. However, this decoupling creates a critical challenge: predicting the model capacity of a given MoE configurations (e.g., expert activation ratio and granularity) remains an unresolved problem. To address this gap, we introduce Efficiency Leverage (EL), a metric quantifying the computational advantage of an MoE model over a dense equivalent. We conduct a large-scale empirical study, training over 300 models up to 28B parameters, to systematically investigate the relationship between MoE architectural configurations and EL. Our findings reveal that EL is primarily driven by the expert activation ratio and the total compute budget, both following predictable power laws, while expert granularity acts as a non-linear modulator with a clear optimal range. We integrate these discoveries into a unified scaling law that accurately predicts the EL of an MoE architecture based on its configuration. To validate our derived scaling laws, we designed and trained Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active parameters, alongside a 6.1B dense model for comparison. When trained on an identical 1T high-quality token dataset, Ling-mini-beta matched the performance of the 6.1B dense model while consuming over 7x fewer computational resources, thereby confirming the accuracy of our scaling laws. This work provides a principled and empirically-grounded foundation for the scaling of efficient MoE models.},
author = {Changxin Tian and Kunlong Chen and Jia Liu and Ziqi Liu and Zhiqiang Zhang and Jun Zhou},
journal = {ArXiv},
volume = {abs/2507.17702},
pages = {null},
doi = {10.48550/arXiv.2507.17702},
arxivid = {2507.17702},
}

@article{db633c6b1c286c0386f0078d8a2e6224e03a6227,
title = {Mistral 7B},
year = {2023},
url = {https://www.semanticscholar.org/paper/db633c6b1c286c0386f0078d8a2e6224e03a6227},
abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
author = {Albert Qiaochu Jiang and Alexandre Sablayrolles and A. Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de Las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L√©lio Renard Lavaud and M. Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth√©e Lacroix and William El Sayed},
journal = {ArXiv},
volume = {abs/2310.06825},
pages = {null},
doi = {10.48550/arXiv.2310.06825},
arxivid = {2310.06825},
}

@article{c6f896aa698b2d65160372bce057ea5f081904de,
title = {LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models},
year = {2025},
url = {https://www.semanticscholar.org/paper/c6f896aa698b2d65160372bce057ea5f081904de},
abstract = {While Masked Diffusion Models (MDMs), such as LLaDA, present a promising paradigm for language modeling, there has been relatively little effort in aligning these models with human preferences via reinforcement learning. The challenge primarily arises from the high variance in Evidence Lower Bound (ELBO)-based likelihood estimates required for preference optimization. To address this issue, we propose Variance-Reduced Preference Optimization (VRPO), a framework that formally analyzes the variance of ELBO estimators and derives bounds on both the bias and variance of preference optimization gradients. Building on this theoretical foundation, we introduce unbiased variance reduction strategies, including optimal Monte Carlo budget allocation and antithetic sampling, that significantly improve the performance of MDM alignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA, and the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor consistently and significantly across mathematical (GSM8K +4.7), code (HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard +4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical performance compared to strong language MDMs and ARMs. Project page: https://ml-gsai.github.io/LLaDA-1.5-Demo/.},
author = {Fengqi Zhu and Rongzheng Wang and Shen Nie and Xiaolu Zhang and Chunwei Wu and Jun Hu and Jun Zhou and Jianfei Chen and Yankai Lin and Ji-Rong Wen and Chongxuan Li},
journal = {ArXiv},
volume = {abs/2505.19223},
pages = {null},
doi = {10.48550/arXiv.2505.19223},
arxivid = {2505.19223},
}

@article{f6e39cee044a03747fbb4b20e335b640b43b884c,
title = {Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs},
year = {2025},
url = {https://www.semanticscholar.org/paper/f6e39cee044a03747fbb4b20e335b640b43b884c},
abstract = {We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.},
author = {Abdelrahman Abouelenin and Atabak Ashfaq and Adam Atkinson and H. Awadalla and Nguyen Bach and Jianmin Bao and A. Benhaim and Martin Cai and Vishrav Chaudhary and Congcong Chen and Dongdong Chen and Dongdong Chen and Junkun Chen and Weizhu Chen and Yen-Chun Chen and Yi-ling Chen and Qi Dai and Xiyang Dai and Ruchao Fan and Mei Gao and Mingcheng Gao and Amit Garg and Abhishek Goswami and Junheng Hao and Amr Hendy and Yuxuan Hu and Xin Jin and Mahmoud Khademi and Dongwoo Kim and Young Jin Kim and Gina Lee and Jinyu Li and Yun-Meng Li and Chen Liang and Xihui Lin and Zeqi Lin and Meng-Jie Liu and Yang Liu and Gilsinia Lopez and Chong Luo and Piyush Madan and V. Mazalov and Ali Mousavi and Anh Nguyen and Jing Pan and D. Perez-Becker and Jacob Platin and Thomas Portet and Kai Qiu and Bo Ren and Liliang Ren and Sambuddha Roy and Ning Shang and Yelong Shen and Saksham Singhal and Subhojit Som and Xiaocheng Song and Tetyana Sych and Praneetha Vaddamanu and Shuohang Wang and Yiming Wang and Zhenghao Wang and Haibin Wu and Haoran Xu and Weijian Xu and Yifan Yang and Ziyi Yang and Donghan Yu and I. Zabir and Jianwen Zhang and L. Zhang and Yunan Zhang and Xiren Zhou},
journal = {ArXiv},
volume = {abs/2503.01743},
pages = {null},
doi = {10.48550/arXiv.2503.01743},
arxivid = {2503.01743},
}

@article{6dc4fe37b965ff8f987ea29c568089da47c1d048,
title = {FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion},
year = {2025},
url = {https://www.semanticscholar.org/paper/6dc4fe37b965ff8f987ea29c568089da47c1d048},
abstract = {We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0.},
author = {Ziyi Yang and Fanqi Wan and Longguang Zhong and Canbin Huang and Guosheng Liang and Xiaojun Quan},
journal = {ArXiv},
volume = {abs/2503.04222},
pages = {null},
doi = {10.48550/arXiv.2503.04222},
arxivid = {2503.04222},
}

@article{3997d41786918a150e15af8f22a4dec57f538fe6,
title = {Every Sample Matters: Leveraging Mixture-of-Experts and High-Quality Data for Efficient and Accurate Code LLM},
year = {2025},
url = {https://www.semanticscholar.org/paper/3997d41786918a150e15af8f22a4dec57f538fe6},
abstract = {Recent advancements in code large language models (LLMs) have demonstrated remarkable capabilities in code generation and understanding. It is still challenging to build a code LLM with comprehensive performance yet ultimate efficiency. Many attempts have been released in the open source community to break the trade-off between performance and efficiency, such as the Qwen Coder series and the DeepSeek Coder series. This paper introduces yet another attempt in this area, namely Ling-Coder-Lite. We leverage the efficient Mixture-of-Experts (MoE) architecture along with a set of high-quality data curation methods (especially those based on program analytics) to build an efficient yet powerful code LLM. Ling-Coder-Lite exhibits on-par performance on 12 representative coding benchmarks compared to state-of-the-art models of similar size, such as Qwen2.5-Coder-7B and DeepSeek-Coder-V2-Lite, while offering competitive latency and throughput. In practice, we achieve a 50\% reduction in deployment resources compared to the similar-sized dense model without performance loss. To facilitate further research and development in this area, we open-source our models as well as a substantial portion of high-quality data for the annealing and post-training stages. The models and data can be accessed at~\url{https://huggingface.co/inclusionAI/Ling-Coder-lite}.},
author = {Codefuse and Wenting Cai and Yuchen Cao and Chaoyu Chen and Chen Chen and Siba Chen and Qing Cui and Peng Di and Junpeng Fang and Zi Gong and Ting Guo and Zhengyu He and Yang Huang and Cong Li and Jianguo Li and Zheng Li and Shijie Lian and Bingchang Liu and Songshan Luo and Shuo Mao and Min Shen and Jian Wu and Jiaolong Yang and Wenjie Yang and Tong Ye and Hang Yu and Wei Zhang and Zhenduo Zhang and Hailin Zhao and Xunjin Zheng and Jun Zhou},
journal = {ArXiv},
volume = {abs/2503.17793},
pages = {null},
doi = {10.48550/arXiv.2503.17793},
arxivid = {2503.17793},
}

@article{6ebcbb71e8cc3e9ab7397fa71eb4a1134116d721,
title = {Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity},
year = {2025},
url = {https://www.semanticscholar.org/paper/6ebcbb71e8cc3e9ab7397fa71eb4a1134116d721},
abstract = {The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price of execution cost for a much larger model parameter count and learning capacity, because only a small fraction of parameters are activated for each input token. However, it is commonly observed that some experts are activated far more often than others, leading to system inefficiency when running the experts on different devices in parallel. Therefore, we introduce Mixture of Grouped Experts (MoGE), which groups the experts during selection and balances the expert workload better than MoE in nature. It constrains tokens to activate an equal number of experts within each predefined expert group. When a model execution is distributed on multiple devices, this architectural design ensures a balanced computational load across devices, significantly enhancing throughput, particularly for the inference phase. Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE with 72 billion total parameters, 16 billion of which are activated for each token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2 through extensive system simulation studies. Our experiments indicate that MoGE indeed leads to better expert load balancing and more efficient execution for both model training and inference on Ascend NPUs. The inference performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further improved to 1528 tokens/s per card by speculative acceleration, outperforming comparable 32B and 72B Dense models. Furthermore, we achieve an excellent cost-to-performance ratio for model inference on Ascend 300I Duo. Our studies show that Ascend NPUs are capable of training Pangu Pro MoE with massive parallelization to make it a leading model within the sub-100B total parameter class, outperforming prominent open-source models like GLM-Z1-32B and Qwen3-32B.},
author = {Yehui Tang and Xiaosong Li and Fangcheng Liu and Wei Guo and Hang Zhou and Yaoyuan Wang and Kai Han and Xian Yu and Jinpeng Li and Hui Zang and Fei Mi and Xiaojun Meng and Zhicheng Liu and Hanting Chen and Binfan Zheng and Can Chen and Youliang Yan and Ruiming Tang and Peifeng Qin and Xinghao Chen and Dacheng Tao and Yunhe Wang},
journal = {ArXiv},
volume = {abs/2505.21411},
pages = {null},
doi = {10.48550/arXiv.2505.21411},
arxivid = {2505.21411},
}

@article{e6accc69f64ff38d85388b4fca7d10c79920274b,
title = {Compass-V2 Technical Report},
year = {2025},
url = {https://www.semanticscholar.org/paper/e6accc69f64ff38d85388b4fca7d10c79920274b},
abstract = {Predominant LLMs focus on high-resource languages while leaving low-resource languages, particularly those in Southeast Asia (SEA), underrepresented. In addition, those models are general-purpose and pay limited attention to the e-commerce domain. To overcome these limitations, we introduce Compass-v2, a lightweight Mixture-of-Experts (MoE) model specifically designed for Southeast Asian languages and e-commerce applications. To balance model performance and inference cost, the model is designed with 30B total parameters and 5B active parameters, incorporating both fine-grained and shared expert modules. To enhance multilingual performance, we curated and constructed a high-quality, industry-leading SEA dataset, to the best of our knowledge. To boost performance in the e-commerce domain, we built a dataset comprising hundreds of billions of tokens, sourced through external data mining and internal platform collection. Besides, we pioneered a hybrid reasoning model that supports both fast thinking and deep thinking within a unified framework to enhance the reasoning capabilities, diverging from the conventional industry practice of deploying two separate models. Through extensive experimental evaluations, our model demonstrates state-of-the-art SEA multilingual and e-commerce performance among sub-30B models, while maintaining significantly lower inference cost.},
author = {Sophia Maria},
journal = {ArXiv},
volume = {abs/2504.15527},
pages = {null},
doi = {10.48550/arXiv.2504.15527},
arxivid = {2504.15527},
}

@article{9150eccda4ba7518d9a5ac670568cc207a5e0e88,
title = {Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs},
year = {2025},
url = {https://www.semanticscholar.org/paper/9150eccda4ba7518d9a5ac670568cc207a5e0e88},
abstract = {We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.},
author = {Yichun Yin and Wenyong Huang and Kaikai Song and Yehui Tang and Xue-Fei Wu and Wei Guo and Peng Guo and Yaoyuan Wang and Xiaojun Meng and Yasheng Wang and Dong Li and Can Chen and Dandan Tu and Yin Li and Fisher Yu and Ruiming Tang and Yunhe Wang and Baojun Wang and Bin Wang and Bo Wang and Boxiao Liu and Changzheng Zhang and Duyu Tang and Fei Mi and Hui Jin and Jiansheng Wei and Jiarui Qin and Jinpeng Li and Jun Zhao and Li-Qiong Deng and Lin Li and Minghui Xu and Naifu Zhang and Nianzu Zheng and Qiang Li and Rongju Ruan and Shen Cheng and Tianyu Guo and Wei He and Wei Li and Wei-Wei Liu and Wu-Peng Liu and Xin Dai and Yong Dong and Yu Pan and Yue Li and Yufei Wang and Yujun Li and Yunsheng Ni and Zhe Liu and Zhenhe Zhang and Zhicheng Liu},
journal = {ArXiv},
volume = {abs/2504.07866},
pages = {null},
doi = {10.48550/arXiv.2504.07866},
arxivid = {2504.07866},
}

@article{e5f56092cbe315ef0177c3e24332f3d2a07fac18,
title = {Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought},
year = {2025},
url = {https://www.semanticscholar.org/paper/e5f56092cbe315ef0177c3e24332f3d2a07fac18},
abstract = {As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It synergistically combines Mamba's long-sequence processing efficiency with Transformer's superior contextual understanding. Hunyuan-TurboS features an adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching between rapid responses for simple queries and deep"thinking"modes for complex problems, optimizing computational resources. Architecturally, this 56B activated (560B total) parameter model employs 128 layers (Mamba2, Attention, FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE structure. Pre-trained on 16T high-quality tokens, it supports a 256K context length and is the first industry-deployed large-scale Mamba model. Our comprehensive post-training strategy enhances capabilities via Supervised Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method, Multi-round Deliberation Learning for iterative improvement, and a two-stage Large-scale Reinforcement Learning process targeting STEM and general instruction-following. Evaluations show strong performance: overall top 7 rank on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances high performance and efficiency, offering substantial capabilities at lower inference costs than many reasoning models, establishing a new paradigm for efficient large-scale pre-trained models.},
author = {Tencent Hunyuan Team Ao Liu and Botong Zhou and Can Xu and Chayse Zhou and Chenchen Zhang and Chengcheng Xu and Chenhao Wang and Decheng Wu and Dengpeng Wu and Dian Jiao and Dong Du and Dong Wang and Feng Zhang and Fengzong Lian and Guanghui Xu and Guanwei Zhang and Hai Wang and Haipeng Luo and Han Hu and Huilin Xu and Jiajia Wu and Jianchen Zhu and Jianfeng Yan and Jiaqi Zhu and Jihong Zhang and Jinbao Xue and Jun Xia and Junqiang Zheng and Kai Liu and Kai Zhang and Kai Zheng and Kejiao Li and Keyao Wang and Lan Jiang and Lixin Liu and Lulu Wu and Meng-Sheng Huang and Peijie Yu and Peiqi Wang and Qian Wang and Qianbiao Xiang and Qibin Liu and Qingfeng Sun and Richard Guo and Ruobing Xie and Saiyong Yang and Shaohua Chen and Shi-He Hu and Shuaipeng Li and Shuaipeng Li and Xinhua Feng and Xinting Huang and Xinyu Guan and Xirui Li and Xu Zhang and Xudong Gao and Xun Luo and Xuxiang Qi and Yangkun Chen and Ya-Li Xiao and Yantao Mai and Yanze Chen and Yao Ding and Yeting Yang and Yifan Song and Yifan Yang and Yijiao Zhu and Yinhe Wu and Yixian Liu and Yong-Guang Yang and Yu-Tong Cai and Yuanlin Tu and Yue Zhang and Yufei Huang and Yuhang Zhou and Yuhao Jiang and Yuhong Liu and Yuhui Hu and Yujin Lin and Yun Yang and Yunhao Wang and Yusong Zhang and Ze-Nan Wu and Zelong Zhang and Zhan Yu and Zhao-Hui Yang and Zheng Li and Zhenyu Huang and Zhiguang Liu and Zhi-Hui Xu and Zhiqing Kui and Zhiyin Zeng and Zhi Xiong and Zhuo Han and Zi-Yin Wu and Zigang Geng and Zilong Zhao and Ziyan Tang and Ziyuan Zhu and Zonglei Zhu},
journal = {ArXiv},
volume = {abs/2505.15431},
pages = {null},
doi = {10.48550/arXiv.2505.15431},
arxivid = {2505.15431},
}

@article{b359b8f736d246312cf9afab7719f4e03928d7e4,
title = {dots.llm1 Technical Report},
year = {2025},
url = {https://www.semanticscholar.org/paper/b359b8f736d246312cf9afab7719f4e03928d7e4},
abstract = {Mixture of Experts (MoE) models have emerged as a promising paradigm for scaling language models efficiently by activating only a subset of parameters for each input token. In this report, we present dots.llm1, a large-scale MoE model that activates 14B parameters out of a total of 142B parameters, delivering performance on par with state-of-the-art models while reducing training and inference costs. Leveraging our meticulously crafted and efficient data processing pipeline, dots.llm1 achieves performance comparable to Qwen2.5-72B after pretraining on 11.2T high-quality tokens and post-training to fully unlock its capabilities. Notably, no synthetic data is used during pretraining. To foster further research, we open-source intermediate training checkpoints at every one trillion tokens, providing valuable insights into the learning dynamics of large language models.},
author = {Bi Huo and Bin Tu and Cheng Qin and Da Zheng and Debing Zhang and Dongjie Zhang and En Li and Fu Guo and Jian Yao and Jie Lou and Junfeng Tian and Li Hu and Ran Zhu and Shengdong Chen and Shuo Liu and Su Guang and Te Wo and Weijun Zhang and Xiaoming Shi and Xinxin Peng and Xing Wu and Yawen Liu and Yuqiu Ji and Ze Wen and Zhenhai Liu and Zichao Li and Zilong Liao},
journal = {ArXiv},
volume = {abs/2506.05767},
pages = {null},
doi = {10.48550/arXiv.2506.05767},
arxivid = {2506.05767},
}

@article{fd8641f1a7c0ef30d6fb312cddd047d8bc492942,
title = {EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes},
year = {2025},
url = {https://www.semanticscholar.org/paper/fd8641f1a7c0ef30d6fb312cddd047d8bc492942},
abstract = {This technical report introduces EXAONE 4.0, which integrates a Non-reasoning mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5 and the advanced reasoning abilities of EXAONE Deep. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model optimized for high performance, and a small-size 1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates superior performance compared to open-weight models in its class and remains competitive even against frontier-class models. The models are publicly available for research purposes and can be easily downloaded via https://huggingface.co/LGAI-EXAONE.},
author = {LG AI Research Kyunghoon Bae and Eunbi Choi and Kibong Choi and Stanley Jungkyu Choi and Yemuk Choi and Kyubeen Han and Seokhee Hong and Junwon Hwang and Tae-wan Hwang and Joonwon Jang and Hyojin Jeon and Kijeong Jeon and Gerrard Jeongwon Jo and Hyunjik Jo and Jiyeon Jung and Euisoon Kim and Hyosang Kim and Jihoon Kim and Joonkee Kim and Seonghwan Kim and Soyeon Kim and SunKyoung Kim and Yireun Kim and Yongil Kim and Youchul Kim and Edward Hwayoung Lee and Gwangho Lee and Haeju Lee and Honglak Lee and Jinsik Lee and Kyungmin Lee and Sang-Goo Park and Young Min Paik and Yongmin Park and Youngy-ong Park and Sanghyun Seo and Sihoon Yang and Heuiy-een Yeen and Sihyuk Yi and Hyeongu Yun},
journal = {ArXiv},
volume = {abs/2507.11407},
pages = {null},
doi = {10.48550/arXiv.2507.11407},
arxivid = {2507.11407},
}

@article{075ed3f43edc114b1942ee8f98a0894a16fee3f5,
title = {Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs},
year = {2025},
url = {https://www.semanticscholar.org/paper/075ed3f43edc114b1942ee8f98a0894a16fee3f5},
abstract = {In this technical report, we tackle the challenges of training large-scale Mixture of Experts (MoE) models, focusing on overcoming cost inefficiency and resource limitations prevalent in such systems. To address these issues, we present two differently sized MoE large language models (LLMs), namely Ling-Lite and Ling-Plus (referred to as"Bailing"in Chinese, spelled B\v{a}il\'ing in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75 billion activated parameters, while Ling-Plus boasts 290 billion parameters with 28.8 billion activated parameters. Both models exhibit comparable performance to leading industry benchmarks. This report offers actionable insights to improve the efficiency and accessibility of AI development in resource-constrained settings, promoting more scalable and sustainable technologies. Specifically, to reduce training costs for large-scale MoE models, we propose innovative methods for (1) optimization of model architecture and training processes, (2) refinement of training anomaly handling, and (3) enhancement of model evaluation efficiency. Additionally, leveraging high-quality data generated from knowledge graphs, our models demonstrate superior capabilities in tool use compared to other models. Ultimately, our experimental findings demonstrate that a 300B MoE LLM can be effectively trained on lower-performance devices while achieving comparable performance to models of a similar scale, including dense and MoE models. Compared to high-performance devices, utilizing a lower-specification hardware system during the pre-training phase demonstrates significant cost savings, reducing computing costs by approximately 20%. The models can be accessed at https://huggingface.co/inclusionAI.},
author = {Ling Team and Binwei Zeng and Chao Huang and Chao Zhang and Changxin Tian and Cong Chen and Dingnan Jin and Fengchun Yu and Feng Zhu and Feng Yuan and Fakang Wang and Gangshan Wang and Guangyao Zhai and Haitao Zhang and Huizhong Li and Jun Zhou and Jia Liu and Junpeng Fang and Junjie Ou and Jun Hu and Ji Luo and Ji Zhang and Jian Liu and Jian Sha and Ji-Kui Qian and Jiewei Wu and Jun-Yu Zhao and Jianguo Li and Jubao Feng and Jing-Rui Di and Jun‚ÄêFa Xu and Jing-Feng Yao and Kuan Xu and Ke Du and Longfei Li and Lei Liang and Lu Yu and Li Tang and Lin Ju and Peng Xu and Qing Cui and Song Liu and Shicheng Li and Shun Song and Song Yan and Tengwei Cai and Tianyi Chen and Ting Guo and Ting Huang and Tao Feng and Tao Wu and Wei Wu and Xiaolu Zhang and Xueming Yang and Xin Zhao and Xiaobo Hu and Xin Lin and Yaodong Zhao and Yilong Wang and Yong-Chang Guo and Yuanyuan Wang and Yue Yang and Yang Cao and Yu-Dong Fu and Yi Xiong and Yanzhe Li and Zhe Li and Zhiqiang Zhang and Ziqi Liu and Zhaoxin Huan and Zujie Wen and Zhenhang Sun and Zhuoxuan Du and Zhengyu He},
journal = {ArXiv},
volume = {abs/2503.05139},
pages = {null},
doi = {10.48550/arXiv.2503.05139},
arxivid = {2503.05139},
}

@article{4f947224264117cc32b1c2cb3dc719ba7f6af3da,
title = {LongCat-Flash Technical Report},
year = {2025},
url = {https://www.semanticscholar.org/paper/4f947224264117cc32b1c2cb3dc719ba7f6af3da},
abstract = {We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research. LongCat Chat: https://longcat.ai Hugging Face: https://huggingface.co/meituan-longcat GitHub: https://github.com/meituan-longcat},
author = {Meituan LongCat Team and Bayan and Bei Li and Bingye Lei and Bo Wang and Bolin Rong and Chao Wang and Chao Zhang and Chen Gao and C. Zhang and Cheng Sun and Chengcheng Han and Chenguang Xi and Chi Zhang and Chong Peng and Chuang Qin and Chuyu Zhang and Cong Chen and Congkui Wang and Dan Ma and Daoru Pan and De-hua Bu and Dengchang Zhao and Deyang Kong and Dishan Liu and Fei Huo and Fengcun Li and Fubao Zhang and Gan Dong and Gang Liu and Gang Xu and Ge Li and Guoqiang Tan and Guo-An Lin and Haihang Jing and Haomin Fu and Haonan Yan and Haoxing Wen and Haozhe Zhao and Hong Liu and Hong-Bin Shi and Hongyan Hao and Hongying Tang and Huantian Lv and Hui Su and Jiacheng Li and Jiahao Liu and Jiahuan Li and Jiajun Yang and Jiaming Wang and Xiao-Tong Shi and Xiaoyu Li and Xili Wang and Xin Chen and Xingyue Hu and Xingyu Miao and Xinyan He and Xuemiao Zhang and Xue‚ÄêLi Hao and Xue-Lei Cao and Xunliang Cai and Xurui Yang and Yan Feng and Yang Bai and Yang Chen and Yang Yang and Yaqi Huo and Yerui Sun and Yifan Lu and Yifan Zhang and Yipeng Zang and Yitao Zhai and Yiyang Li and Yong-hao Yin and Yong-Feng Lv and Yong Zhou and Yu Yang and Yuchen Xie and Yue Sun and Yu-Guo Zheng and Y. Wei and Yulei Qian and Yun-Peng Liang and Yu-Wing Tai and Yunke Zhao and Ze-Xin Yu and Zhao Zhang and Zhaohua Yang and Zhenchao Zhang and Zhi-Wei Xia and Zhiye Zou and Zhizhao Zeng and Zhongda Su and Zhuo Chen and Zijian Zhang and Ziwen Wang and Zixu Jiang and Zi-Hua Zhao and Zongyu Wang and Zunhai Su},
arxivid = {2509.01322},
}

@article{ca31807d39af3f491940f62053bde0b976566fc6,
title = {WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training},
year = {2025},
url = {https://www.semanticscholar.org/paper/ca31807d39af3f491940f62053bde0b976566fc6},
abstract = {Recent advances in learning rate (LR) scheduling have demonstrated the effectiveness of decay-free approaches that eliminate the traditional decay phase while maintaining competitive performance. Model merging techniques have emerged as particularly promising solutions in this domain. We present Warmup-Stable and Merge (WSM), a general framework that establishes a formal connection between learning rate decay and model merging. WSM provides a unified theoretical foundation for emulating various decay strategies-including cosine decay, linear decay and inverse square root decay-as principled model averaging schemes, while remaining fully compatible with diverse optimization methods. Through extensive experiments, we identify merge duration-the training window for checkpoint aggregation-as the most critical factor influencing model performance, surpassing the importance of both checkpoint interval and merge quantity. Our framework consistently outperforms the widely-adopted Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on MMLU-Pro. The performance advantages extend to supervised fine-tuning scenarios, highlighting WSM's potential for long-term model refinement.},
author = {Changxin Tian and Jiapeng Wang and Qian Zhao and Kunlong Chen and Jia Liu and Ziqi Liu and Jiaxin Mao and Wayne Xin Zhao and Zhiqiang Zhang and Jun Zhou},
journal = {ArXiv},
volume = {abs/2507.17634},
pages = {null},
doi = {10.48550/arXiv.2507.17634},
arxivid = {2507.17634},
}

@article{53a803388e83ae89261624099d7be4287ace67cb,
title = {DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
year = {2024},
url = {https://www.semanticscholar.org/paper/53a803388e83ae89261624099d7be4287ace67cb},
abstract = {We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.},
author = {Zhihong Shao and Damai Dai and Daya Guo and Bo Liu (Benjamin Liu) and Zihan Wang and Huajian Xin},
journal = {ArXiv},
volume = {abs/2405.04434},
pages = {null},
doi = {10.48550/arXiv.2405.04434},
arxivid = {2405.04434},
}

@article{cf5f4acfd80e30c141c132322e012bac687c74ab,
title = {AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning},
year = {2025},
url = {https://www.semanticscholar.org/paper/cf5f4acfd80e30c141c132322e012bac687c74ab},
abstract = {Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable.},
author = {Yang Chen and Zhuoling Yang and Zihan Liu and Chankyu Lee and Peng Xu and M. Shoeybi and Bryan Catanzaro and Wei Ping},
journal = {ArXiv},
volume = {abs/2505.16400},
pages = {null},
doi = {10.48550/arXiv.2505.16400},
arxivid = {2505.16400},
}

@article{ef063ff2db936fb7c1b13d2fdd24f04343b6fc72,
title = {Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions},
year = {2024},
url = {https://www.semanticscholar.org/paper/ef063ff2db936fb7c1b13d2fdd24f04343b6fc72},
abstract = {Recent advancements in Transformer-based large language models (LLMs) have set new standards in natural language processing. However, the classical softmax attention incurs significant computational costs, leading to a $O(T)$ complexity for per-token generation, where $T$ represents the context length. This work explores reducing LLMs' complexity while maintaining performance by introducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an innovative data-dependent tempered selection (DDTS) mechanism within a linear attention-based, purely recurrent framework, achieving significant accuracy while drastically reducing the memory usage typically associated with recurrent models. This method exemplifies semantic compression by maintaining essential input information with fixed-size hidden states. Building on this, Rodimus$+$ combines Rodimus with the innovative Sliding Window Shared-Key Attention (SW-SKA) in a hybrid approach, effectively leveraging the complementary semantic, token, and head compression techniques. Our experiments demonstrate that Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior downstream performance against models trained on more tokens, including Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints are open-sourced at https://github.com/codefuse-ai/rodimus.},
author = {Zhihao He and Hang Yu and Zi Gong and Shizhan Liu and Jianguo Li and Weiyao Lin},
journal = {ArXiv},
volume = {abs/2410.06577},
pages = {null},
doi = {10.48550/arXiv.2410.06577},
arxivid = {2410.06577},
}
